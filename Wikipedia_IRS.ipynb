{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkS4Pf-JFEfL"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3fbbY3jn68N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86692335-0652-4a1a-f438-a60398ac37b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=e433697e39e925589b8968aed9b36914746f8b8adcb10745b960f30f19e93ecd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.7.4)\n",
            "Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.8)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.11)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia\n",
        "!pip install wikipedia-api\n",
        "!pip install ipywidgets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# web scraping\n",
        "import wikipedia\n",
        "import wikipediaapi\n",
        "\n",
        "# preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# IRsystem\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# cosine similarity\n",
        "from math import sqrt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# interface\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrLpBm7RFHs4"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGgrO2Qcpph5"
      },
      "outputs": [],
      "source": [
        "greek_latin_letters = {\n",
        "  r'α': 'alpha', r'β': 'beta', r'γ': 'gamma', r'Δ': 'delta',\n",
        "  r'ε': 'epsilon', r'θ': 'theta', r'λ': 'lambda', r'μ': 'mu',\n",
        "  r'π': 'pi', r'σ': 'sigma', r'τ': 'tau', r'υ': 'upsilon',\n",
        "  r'ω': 'omega',\n",
        "  r'Α': 'alpha', r'Β': 'beta', r'Γ': 'gamma', r'Δ': 'delta',\n",
        "  r'Ε': 'epsilon', r'Θ': 'theta', r'Λ': 'lambda', r'Μ': 'mu',\n",
        "  r'Π': 'pi', r'Σ': 'sigma', r'Τ': 'tau', r'Υ': 'upsilon',\n",
        "  r'Ω': 'omega',\n",
        "  r'č': 'c', r'š': 's', r'ž': 'z', r'é': 'e', r'è': 'e', r'á': 'a',\n",
        "  r'à': 'a', r'ç': 'c', r'ú': 'u', r'ó': 'o', r'í': 'i', r'ñ': 'n',\n",
        "  r'ü': 'u', r'ℓ': 'l',\n",
        "  r'Č': 'c', r'Š': 's', r'Ž': 'z', r'É': 'e', r'È': 'e', r'Á': 'a',\n",
        "  r'À': 'a', r'Ç': 'c', r'Ú': 'u', r'Ó': 'o', r'Í': 'i', r'Ñ': 'n',\n",
        "  r'Ü': 'u', r'ℒ': 'l',\n",
        "  r'â': 'a', r'ã': 'a', r'ä': 'a', r'Ä': 'a', r'æ': 'ae', r'Æ': 'ae',\n",
        "  r'ê': 'e', r'ï': 'i', r'ò': 'o', r'ö': 'o', r\"Ö\": 'o',\n",
        "  r'ā': 'a', r'ē': 'e', r'ī': 'i', r'ō': 'o',\n",
        "  r'œ': \"oe\", r'Œ': 'oe', r'ή': 'e', r'ί': 'i', r'ζ': 'z',\n",
        "  r'η': 'e', r'ι': 'i', r'κ': 'k', r'ν': 'n',\n",
        "  r'ο': 'o', r'ρ': 'r', r'ς': 's', r'φ': 'f',\n",
        "  r'ό': 'o', r'ύ': 'u', r'ḗ': 'e', r'ἶ': 'i',\n",
        "  r'ῆ': 'e', r'ῷ': 'o', r'с': 's', r'С': 's'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CINfH63EMmV4"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  # replace artificial intelligence and AIs with ai\n",
        "  text = re.sub(r\"[Aa]rtificial [Ii]ntelligence|\\bAIs\\b|\\bais\\b\", \"ai\", text)\n",
        "  # replace underscore/hyphen with space\n",
        "  text = re.sub(r\"[_\\-—]+\", \" \", text)\n",
        "  # remove 's if at end of word\n",
        "  text = re.sub(r\"('s)\\b\", \"\", text)\n",
        "  # remove s/st/nd/rd/th if it's after a number\n",
        "  text = re.sub(r\"(?<=\\d)(s|st|nd|rd|th)\", r\"\", text)\n",
        "  # add a space before a capital letter if it's in the middle of a word\n",
        "  text = re.sub(r\"(?<=\\[a-z])([A-Z])\", r\" \\1\", text)\n",
        "  # replace '²' and ^2 with squared\n",
        "  text = re.sub(r\"²|\\^2| to the power of 2\", \" squared\", text)\n",
        "  # replace greek/latin letters to become their meaning/how a typical person writes them\n",
        "  for character, replacement in greek_latin_letters.items():\n",
        "      text = re.sub(character, replacement, text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpE3Nt0DY3Zj"
      },
      "outputs": [],
      "source": [
        "def to_sentence(text):\n",
        "  # remove symbols (all characters other than letters/numbers/whitespaces/%/./!/?)\n",
        "  text = re.sub(r\"[^\\w\\s%\\.!?]\", \"\", text)\n",
        "  # remove . if abreviation was before it (e.g.)\n",
        "  text = re.sub(r\"(?<=\\.[A-Za-z])\\.\", \"\", text)\n",
        "  # remove . unless a number or whitespace was after it\n",
        "  text = re.sub(r\"\\.(?![\\d\\s])\", \"\", text)\n",
        "  return re.split(r\"\\.(?!\\S)|[\\n!?]\", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbWD_2nX9Gjg"
      },
      "outputs": [],
      "source": [
        "def word_tokenize(sent):\n",
        "  # remove symbols (all characters other than letters/numbers/whitespaces/%/.)\n",
        "  sent = re.sub(r\"[^\\w\\s%\\.]\", \"\", sent)\n",
        "  # remove . unless a number was right after it\n",
        "  sent = re.sub(r\"\\.(?!\\d)\", \"\", sent)\n",
        "  return re.findall(r\"\\b\\w+\\b\", sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvSWOz85nRJd"
      },
      "outputs": [],
      "source": [
        "stop_words = set(word_tokenize(clean_text(\" \".join(stopwords.words('english')))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBQtz2G55b5g"
      },
      "outputs": [],
      "source": [
        "def is_eng(word):\n",
        "  # arabic\n",
        "  if bool(re.match(r'^[\\u0600-\\u06FF\\s]+$', word)):\n",
        "    return False\n",
        "  # chinese\n",
        "  if bool(re.match(r'^[\\u4E00-\\u9FFF\\s]+$', word)):\n",
        "    return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_lemmatized(sent):\n",
        "  lemmatized_tokens = []\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for word in sent:\n",
        "    lemmatized_token = lemmatizer.lemmatize(word, pos='v')\n",
        "    lemmatized_token = lemmatizer.lemmatize(lemmatized_token, pos='n')\n",
        "    lemmatized_token = lemmatizer.lemmatize(lemmatized_token, pos='a')\n",
        "    lemmatized_tokens.append(lemmatized_token)\n",
        "  return lemmatized_tokens"
      ],
      "metadata": {
        "id": "mpPVH1V6s-C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEqudyQdM7k5"
      },
      "outputs": [],
      "source": [
        "def preprocess(clean):\n",
        "  words = word_tokenize(clean)\n",
        "  lower = [word.lower() for word in words if word.lower() not in stop_words and is_eng(word.lower())]\n",
        "\n",
        "  return \" \".join(make_lemmatized(lower))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVhHHaJYZ7mD"
      },
      "outputs": [],
      "source": [
        "def preprocess_to_sent(clean):\n",
        "  sents = to_sentence(clean)\n",
        "  sents_tokens = [word_tokenize(sent) for sent in sents]\n",
        "  sents_lower = []\n",
        "  for i in range(len(sents_tokens)):\n",
        "    lower_words = [word.lower() for word in sents_tokens[i] if word.lower() not in stop_words and is_eng(word.lower())]\n",
        "    # check if sent is not empty/space (\"\"/\" \")\n",
        "    if lower_words:\n",
        "        sents_lower.append([word for word in lower_words]) #if word is not None])\n",
        "  longest = 0\n",
        "  for sent in sents_lower:\n",
        "    if len(sent) > longest:\n",
        "      longest = len(sent)\n",
        "  sents_lem = [make_lemmatized(sent) for sent in sents_lower]\n",
        "\n",
        "  return sents_lem, longest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgtSz2HH_qQ-"
      },
      "outputs": [],
      "source": [
        "# def analyze_text(text):\n",
        "#   total_words = len(set(words))\n",
        "#   unique_words = len(set(lemmatized_tokens))\n",
        "#   stop_words_count = len(set(stop_words))\n",
        "#   unique_words_percentage = f\"{(unique_words / total_words) * 100:.2f}%\"\n",
        "#   stop_words_percentage = f\"{(stop_words_count / total_words) * 100:.2f}%\"\n",
        "\n",
        "#   print(\"unique words pct:\", unique_words_percentage)\n",
        "#   print(\"stop words pct:\", stop_words_percentage)\n",
        "\n",
        "#   return total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSruQZZ0uDJl"
      },
      "outputs": [],
      "source": [
        "def get_articles(titles):\n",
        "  wiki_wiki = wikipediaapi.Wikipedia('Wikipedia_IRS', 'en')\n",
        "  results = []\n",
        "  longest_all = 0\n",
        "  for title in titles:\n",
        "    result = {}\n",
        "    page = wiki_wiki.page(title)\n",
        "\n",
        "    result[\"URL\"] = page.fullurl\n",
        "    result[\"Title\"] = title\n",
        "    result[\"Text\"] = page.text\n",
        "    result[\"Tokens\"] = preprocess(clean_text(page.text))\n",
        "    result[\"Tokens_list\"], longest = preprocess_to_sent(clean_text(page.text))\n",
        "    if longest > longest_all:\n",
        "      longest_all = longest\n",
        "    results.append(result)\n",
        "\n",
        "  return pd.DataFrame(results), longest_all"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sents(sents, longest):\n",
        "  padded_sents = []\n",
        "  for sent in sents:\n",
        "    padded = sent.copy()\n",
        "    while len(padded) < longest:\n",
        "      padded.append(\"<oov>\")\n",
        "    padded_sents.append(padded)\n",
        "  return padded_sents"
      ],
      "metadata": {
        "id": "xmak9vAabgOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_query_with_synonyms(query):\n",
        "    synonyms = set()\n",
        "    for word in query.split():\n",
        "        for synset in wordnet.synsets(word):\n",
        "            synonyms.update(synset.lemma_names())\n",
        "    return query + ' ' + ' '.join(synonyms)"
      ],
      "metadata": {
        "id": "jK5gljZo6olV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gains(scores):\n",
        "  max = scores.iloc[0][\"score\"]\n",
        "  min = scores.iloc[-1][\"score\"]\n",
        "\n",
        "  r = np.linspace(min, max, num=5)\n",
        "  scores[\"gain\"] = 0\n",
        "  for i in scores.index:\n",
        "    if scores.loc[i,\"score\"] < r[1]:\n",
        "      scores.loc[i,\"gain\"] = 0\n",
        "    elif scores.loc[i,\"score\"] < r[2]:\n",
        "      scores.loc[i,\"gain\"] = 1\n",
        "    elif scores.loc[i,\"score\"] < r[3]:\n",
        "      scores.loc[i,\"gain\"] = 2\n",
        "    elif scores.loc[i,\"score\"] <= r[4]:\n",
        "      scores.loc[i,\"gain\"] = 3\n",
        "  return scores"
      ],
      "metadata": {
        "id": "h8lwp_ov_BFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finalize_df(df):\n",
        "  if df['docID'][0] == 0:\n",
        "    additional_data = {'docID': range(20, 60),\n",
        "                   'ideal_gain': [0] * 40}\n",
        "    df2 = pd.DataFrame(additional_data)\n",
        "\n",
        "    df = pd.concat([df, df2], ignore_index=True)\n",
        "\n",
        "  elif df['docID'][0] == 20:\n",
        "    additional_data = {'docID': range(0, 20),\n",
        "                       'ideal_gain': [0] * 20}\n",
        "    df2 = pd.DataFrame(additional_data)\n",
        "\n",
        "    df = pd.concat([df, df2], ignore_index = True)\n",
        "\n",
        "    additional_data = {'docID': range(40, 60),\n",
        "                       'ideal_gain': [0] * 20}\n",
        "    df2 = pd.DataFrame(additional_data)\n",
        "\n",
        "    df = pd.concat([df, df2], ignore_index = True)\n",
        "\n",
        "  else:\n",
        "    additional_data = {'docID': range(0, 40),\n",
        "                   'ideal_gain': [0] * 40}\n",
        "    df2 = pd.DataFrame(additional_data)\n",
        "\n",
        "    df = pd.concat([df, df2], ignore_index=True)\n",
        "\n",
        "  return df.sort_values(by='docID', ascending=True).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "S6S5Q0ii7DI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_df(expert_df, model_df):\n",
        "    expert_df = expert_df.sort_index()\n",
        "    model_df = model_df.sort_index()\n",
        "    merged_df = pd.merge(expert_df, model_df, left_index=True, right_index=True, how='inner')\n",
        "    return merged_df"
      ],
      "metadata": {
        "id": "2wLILrN3rTvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUObpgCDFKws"
      },
      "source": [
        "#Creating DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEc77sqi4vcL"
      },
      "outputs": [],
      "source": [
        "titles_AI = wikipedia.search(\"Artificial intelligence\", results = 20)\n",
        "titles_DS = wikipedia.search(\"Data science\", results = 20)\n",
        "titles_DB = wikipedia.search(\"Database\", results = 20)\n",
        "titles = titles_AI + titles_DS + titles_DB\n",
        "\n",
        "articles, longest_sent = get_articles(titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMy880vVpMG-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "16dd629c-02fa-409b-cee9-4780e21629e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 URL  \\\n",
              "0  https://en.wikipedia.org/wiki/Artificial_intel...   \n",
              "1  https://en.wikipedia.org/wiki/Generative_artif...   \n",
              "2  https://en.wikipedia.org/wiki/A.I._Artificial_...   \n",
              "3  https://en.wikipedia.org/wiki/Artificial_gener...   \n",
              "4  https://en.wikipedia.org/wiki/Applications_of_...   \n",
              "\n",
              "                                     Title  \\\n",
              "0                  Artificial intelligence   \n",
              "1       Generative artificial intelligence   \n",
              "2             A.I. Artificial Intelligence   \n",
              "3          Artificial general intelligence   \n",
              "4  Applications of artificial intelligence   \n",
              "\n",
              "                                                Text  \\\n",
              "0  Artificial intelligence (AI), in its broadest ...   \n",
              "1  Generative artificial intelligence (generative...   \n",
              "2  A.I. Artificial Intelligence (or simply A.I.) ...   \n",
              "3  Artificial general intelligence (AGI) is a typ...   \n",
              "4  Artificial intelligence (AI) has been used in ...   \n",
              "\n",
              "                                              Tokens  \\\n",
              "0  ai ai broad sense intelligence exhibit machine...   \n",
              "1  generative ai generative ai genai gai ai capab...   \n",
              "2  ai ai simply ai 2001 american science fiction ...   \n",
              "3  artificial general intelligence agi type ai ai...   \n",
              "4  ai ai use application throughout industry acad...   \n",
              "\n",
              "                                         Tokens_list  \n",
              "0  [[ai, ai, broad, sense, intelligence, exhibit,...  \n",
              "1  [[generative, ai, generative, ai, genai, gai, ...  \n",
              "2  [[ai, ai, simply, ai, 2001, american, science,...  \n",
              "3  [[artificial, general, intelligence, agi, type...  \n",
              "4  [[ai, ai, use, application, throughout, indust...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c52d7fe1-af16-4ee4-a5cf-b1a0b0164b9f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL</th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Tokens_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://en.wikipedia.org/wiki/Artificial_intel...</td>\n",
              "      <td>Artificial intelligence</td>\n",
              "      <td>Artificial intelligence (AI), in its broadest ...</td>\n",
              "      <td>ai ai broad sense intelligence exhibit machine...</td>\n",
              "      <td>[[ai, ai, broad, sense, intelligence, exhibit,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://en.wikipedia.org/wiki/Generative_artif...</td>\n",
              "      <td>Generative artificial intelligence</td>\n",
              "      <td>Generative artificial intelligence (generative...</td>\n",
              "      <td>generative ai generative ai genai gai ai capab...</td>\n",
              "      <td>[[generative, ai, generative, ai, genai, gai, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://en.wikipedia.org/wiki/A.I._Artificial_...</td>\n",
              "      <td>A.I. Artificial Intelligence</td>\n",
              "      <td>A.I. Artificial Intelligence (or simply A.I.) ...</td>\n",
              "      <td>ai ai simply ai 2001 american science fiction ...</td>\n",
              "      <td>[[ai, ai, simply, ai, 2001, american, science,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://en.wikipedia.org/wiki/Artificial_gener...</td>\n",
              "      <td>Artificial general intelligence</td>\n",
              "      <td>Artificial general intelligence (AGI) is a typ...</td>\n",
              "      <td>artificial general intelligence agi type ai ai...</td>\n",
              "      <td>[[artificial, general, intelligence, agi, type...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://en.wikipedia.org/wiki/Applications_of_...</td>\n",
              "      <td>Applications of artificial intelligence</td>\n",
              "      <td>Artificial intelligence (AI) has been used in ...</td>\n",
              "      <td>ai ai use application throughout industry acad...</td>\n",
              "      <td>[[ai, ai, use, application, throughout, indust...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c52d7fe1-af16-4ee4-a5cf-b1a0b0164b9f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c52d7fe1-af16-4ee4-a5cf-b1a0b0164b9f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c52d7fe1-af16-4ee4-a5cf-b1a0b0164b9f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-19554688-ef24-4912-9955-5a67f0057dfd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-19554688-ef24-4912-9955-5a67f0057dfd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-19554688-ef24-4912-9955-5a67f0057dfd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "articles",
              "summary": "{\n  \"name\": \"articles\",\n  \"rows\": 60,\n  \"fields\": [\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n          \"https://en.wikipedia.org/wiki/History_of_artificial_intelligence\",\n          \"https://en.wikipedia.org/wiki/Data_Science_Institute\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"Artificial intelligence\",\n          \"History of artificial intelligence\",\n          \"Data Science Institute\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nSome high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, Apple Intelligence, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \\\"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\\\"\\nAlan Turing was the first person to conduct substantial research in the field that he called \\\"machine intelligence\\\". Artificial intelligence was founded as an academic discipline in 1956, by those now considered the founding fathers of AI: John McCarthy, Marvin Minksy, Nathaniel Rochester, and Claude Shannon. The field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.\\nThe growing use of artificial intelligence in the 21st century is influencing a societal and economic shift towards increased automation, data-driven decision-making, and the integration of AI systems into various economic sectors and areas of life, impacting job markets, healthcare, government, industry, education, propaganda, and disinformation. This raises questions about the long-term effects, ethical implications, and risks of AI, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence\\u2014the ability to complete any task performable by a human on an at least equal level\\u2014is among the field's long-term goals.\\nTo reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\n\\nGoals\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\nReasoning and problem-solving\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \\\"combinatorial explosion\\\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\nKnowledge representation\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \\\"interesting\\\" and actionable inferences from large databases), and other areas.\\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \\\"facts\\\" or \\\"statements\\\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\\n\\nPlanning and decision-making\\nAn \\\"agent\\\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences\\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \\\"utility\\\") that measures how much the agent prefers it. For each possible action, it can calculate the \\\"expected utility\\\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \\\"unknown\\\" or \\\"unobservable\\\") and it may not know for certain what will happen after each possible action (it is not \\\"deterministic\\\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\\n\\nLearning\\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \\\"good\\\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\\n\\nNatural language processing\\nNatural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \\\"micro-worlds\\\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \\\"GPT\\\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\\n\\nPerception\\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\\nThe field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception.\\n\\nSocial intelligence\\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\\u2013computer interaction.\\nHowever, this tends to give na\\u00efve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.\\n\\nGeneral intelligence\\nA machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\\n\\nTechniques\\nAI research uses a wide variety of techniques to accomplish the goals above.\\n\\nSearch and optimization\\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\\n\\nState space search\\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \\\"Heuristics\\\" or \\\"rules of thumb\\\" can help prioritize choices that are more likely to reach a goal.\\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.\\n\\nLocal search\\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.\\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \\\"mutating\\\" and \\\"recombining\\\" them, selecting only the fittest to survive each generation.\\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\\n\\nLogic\\nFormal logic is used for reasoning and knowledge representation.\\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \\\"and\\\", \\\"or\\\", \\\"not\\\" and \\\"implies\\\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \\\"Every X is a Y\\\" and \\\"There are some Xs that are Ys\\\").\\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\\nFuzzy logic assigns a \\\"degree of truth\\\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.\\nOther specialized versions of logic have been developed to describe many complex domains.\\n\\nProbabilistic methods for uncertain reasoning\\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation\\u2013maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\\n\\nClassifiers and statistical learning methods\\nThe simplest AI applications can be divided into two types: classifiers (e.g., \\\"if shiny then diamond\\\"), on one hand, and controllers (e.g., \\\"if diamond then pick up\\\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \\\"observation\\\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\\nThe naive Bayes classifier is reportedly the \\\"most widely used learner\\\" at Google, due in part to its scalability.\\nNeural networks are also used as classifiers.\\n\\nArtificial neural networks\\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.\\nNeural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\\nIn feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.\\nPerceptrons\\nuse only a single layer of neurons, deep learning uses multiple layers.\\nConvolutional neural networks strengthen the connection between neurons that are \\\"close\\\" to each other\\u2014this is especially important in image processing, where a local set of neurons must identify an \\\"edge\\\" before the network can identify an object.\\n\\nDeep learning\\nDeep learning\\nuses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023.\\nThe sudden success of deep learning in 2012\\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)\\nbut because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\\n\\nGPT\\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that are based on the semantic relationships between words in sentences (natural language processing). Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \\\"hallucinations\\\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.\\nCurrent models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\\n\\nSpecialized hardware and software\\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\\n\\nApplications\\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\\n\\nHealth and medicine\\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\\n\\nGames\\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then in 2017 it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\\n\\nMathematics\\nIn mathematics, special forms of formal step-by-step reasoning are used. In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.\\nAlternatively, dedicated models for mathematic problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind, Llemma from eleuther or Julius.\\nWhen natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematic tasks.\\nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\\n\\nFinance\\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \\\"robot advisers\\\" have been in use for some years.\\n\\nWorld Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \\\"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I\\u2019m not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\\\"\\n\\nMilitary\\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.\\nIn November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.\\n\\nGenerative AI\\nIn the early 2020s, generative AI gained widespread prominence. In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.\\n\\nOther industry-specific tasks\\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \\\"AI\\\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\\nAI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.\\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \\\"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\\\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\\n\\nEthics\\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \\\"solve intelligence, and then use that to solve everything else\\\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\\n\\nRisks and harm\\nPrivacy and copyright\\nMachine-learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\\nTechnology companies collect a wide range of data from their users, including online activity, geolocation data, video and audio.\\nFor example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\\nAI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \\\"from the question of 'what they know' to the question of 'what they're doing with it'.\\\"\\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \\\"fair use\\\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \\\"the purpose and character of the use of the copyrighted work\\\" and \\\"the effect upon the potential market for the copyrighted work\\\". Website owners who do not wish to have their content scraped can indicate it in a \\\"robots.txt\\\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\\n\\nDominance by tech giants\\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\\n\\nSubstantial power needs and other environmental impacts\\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\\nProdigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \\u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \\u2013 in the long view \\u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \\\"intelligent\\\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \\\"US power demand (is) likely to experience growth not seen in a generation\\u2026.\\\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).\\n\\nMisinformation\\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .\\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \\\"authoritarian leaders to manipulate their electorates\\\" on a large scale, among other risks.\\n\\nAlgorithmic bias and fairness\\nIn statistics, a bias is a systematic error or deviation from the correct value. But in the context of fairness, it often refers to a tendency in favor or against a certain group or individual characteristic, usually in a way that is considered unfair or harmful. A statistically unbiased AI system that produces disparate outcomes for different demographic groups may thus be viewed as biased in the ethical sense.\\nThe field of fairness studies how to prevent harms from algorithmic biases. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems don't reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists.\\nBias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.\\nOn June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \\\"gorillas\\\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \\\"sample size disparity\\\". Google \\\"fixed\\\" this problem by preventing the system from labelling anything as a \\\"gorilla\\\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.\\nIn 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \\\"race\\\" or \\\"gender\\\"). The feature will correlate with other features (like \\\"address\\\", \\\"shopping history\\\" or \\\"first name\\\"), and the program will make the same decisions based on these features as it would on \\\"race\\\" or \\\"gender\\\".\\nMoritz Hardt said \\\"the most robust fact in this research area is that fairness through blindness doesn't work.\\\"\\nCriticism of COMPAS highlighted that machine learning models are designed to make \\\"predictions\\\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \\\"recommendations\\\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\\n\\nLack of transparency\\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \\\"cancerous\\\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \\\"low risk\\\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\\nDARPA established the XAI (\\\"Explainable Artificial Intelligence\\\") program in 2014 to try and solve these problems.\\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\\n\\nBad actors and weaponized AI\\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier\\u2014AI facial recognition systems are already being used for mass surveillance in China.\\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\\n\\nTechnological unemployment\\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \\\"we're in uncharted territory\\\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \\\"high risk\\\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \\\"high risk\\\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \\\"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\\\" is \\\"worth taking seriously\\\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\\n\\nExistential risk\\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \\\"spell the end of the human race\\\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \\\"self-awareness\\\" (or \\\"sentience\\\" or \\\"consciousness\\\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\\nFirst, AI does not require human-like \\\"sentience\\\" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \\\"you can't fetch the coffee if you're dead.\\\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \\\"fundamentally on our side\\\".\\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \\\"freely speak out about the risks of AI\\\" without \\\"considering how this impacts Google.\\\" He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\\nIn 2023, many leading AI experts issued the joint statement that \\\"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\\\".\\nOther researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \\\"human lives longer and healthier and easier.\\\" While the tools that are now being used to improve lives can also be used by bad actors, \\\"they can also be used against the bad actors.\\\" Andrew Ng also argued that \\\"it's a mistake to fall for the doomsday hype on AI\\u2014and that regulators who do will only benefit vested interests.\\\" Yann LeCun \\\"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\\\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\\n\\nEthical machines and alignment\\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\\nThe field of machine ethics is also called computational morality,\\nand was founded at an AAAI symposium in 2005.\\nOther approaches include Wendell Wallach's \\\"artificial moral agents\\\" and Stuart J. Russell's three principles for developing provably beneficial machines.\\n\\nOpen source\\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \\\"weights\\\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they can't be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\\n\\nFrameworks\\nArtificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values\\u2014developed by the Alan Turing Institute tests projects in four main areas:\\n\\nRespect the dignity of individual people\\nConnect with other people sincerely, openly, and inclusively\\nCare for the wellbeing of everyone\\nProtect social values, justice, and the public interest\\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.\\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\\n\\nRegulation\\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.\\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \\\"products and services using AI have more benefits than drawbacks\\\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \\\"very important\\\", and an additional 41% thought it \\\"somewhat important\\\", for the federal government to regulate AI, versus 13% responding \\\"not very important\\\" and 8% responding \\\"not at all important\\\".\\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\\n\\nHistory\\nThe study of mechanical or \\\"formal\\\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \\\"0\\\" and \\\"1\\\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \\\"electronic brain\\\".\\nThey developed several areas of research that would become part of AI,\\nsuch as McCullouch and Pitts design for \\\"artificial neurons\\\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \\\"machine intelligence\\\" was plausible.\\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \\\"astonishing\\\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \\\"machines will be capable, within twenty years, of doing any work a man can do\\\". In 1967 Marvin Minsky agreed, writing, \\\"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\\\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \\\"AI winter\\\", a period when obtaining funding for AI projects was difficult, followed.\\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \\\"sub-symbolic\\\" approaches. Rodney Brooks rejected \\\"representation\\\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \\\"connectionism\\\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \\\"narrow\\\" and \\\"formal\\\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \\\"artificial intelligence\\\".\\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \\\"AGI\\\"), which had several well-funded institutions by the 2010s.\\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\\nFor many specific tasks, other methods were abandoned.\\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\\u20132019.\\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\\nIn the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in \\\"AI\\\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \\\"AI\\\".\\nAbout 800,000 \\\"AI\\\"-related U.S. job openings existed in 2022.\\n\\nPhilosophy\\nDefining artificial intelligence\\nAlan Turing wrote in 1950 \\\"I propose to consider the question 'can machines think'?\\\" He advised changing the question from whether a machine \\\"thinks\\\", to \\\"whether or not it is possible for machinery to show intelligent behaviour\\\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \\\"actually\\\" thinking or literally has a \\\"mind\\\". Turing notes that we can not determine these things about other people but \\\"it is usual to have a polite convention that everyone thinks.\\\"\\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \\\"Aeronautical engineering texts,\\\" they wrote, \\\"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\\\" AI founder John McCarthy agreed, writing that \\\"Artificial intelligence is not, by definition, simulation of human intelligence\\\".\\nMcCarthy defines intelligence as \\\"the computational part of the ability to achieve goals in the world\\\". Another AI founder, Marvin Minsky similarly describes it as \\\"the ability to solve hard problems\\\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \\\"intelligence\\\" of the machine\\u2014and no other philosophical discussion is required, or may not even be possible.\\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\\nSome authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \\\"not actually use AI in a material way\\\".\\n\\nEvaluating approaches to AI\\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \\\"artificial intelligence\\\" to mean \\\"machine learning with neural networks\\\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\\n\\nSymbolic AI and its limits\\nSymbolic AI (or \\\"GOFAI\\\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \\\"intelligent\\\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \\\"A physical symbol system has the necessary and sufficient means of general intelligent action.\\\"\\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \\\"intelligent\\\" tasks were easy for AI, but low level \\\"instinctive\\\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \\\"feel\\\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\\n\\nNeat vs. scruffy\\n\\\"Neats\\\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \\\"Scruffies\\\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\\n\\nSoft vs. hard computing\\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\\n\\nNarrow vs. general AI\\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\\n\\nMachine consciousness, sentience, and mind\\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \\\"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\\\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\\n\\nConsciousness\\nDavid Chalmers identified two problems in understanding the mind, which he named the \\\"hard\\\" and \\\"easy\\\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\\n\\nComputationalism and functionalism\\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\\nPhilosopher John Searle characterized this position as \\\"strong AI\\\": \\\"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\\\" Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\\n\\nAI welfare and rights\\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\\nIn 2017, the European Union considered granting \\\"electronic personhood\\\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.\\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\\n\\nFuture\\nSuperintelligence and the singularity\\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.\\nIf research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \\\"intelligence explosion\\\" and Vernor Vinge called a \\\"singularity\\\".\\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\\n\\nTranshumanism\\nRobot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.\\nEdward Fredkin argues that \\\"artificial intelligence is the next stage in evolution\\\", an idea first proposed by Samuel Butler's \\\"Darwin among the Machines\\\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\\n\\nIn fiction\\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \\\"Multivac\\\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \\u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\\n\\nSee also\\nArtificial intelligence detection software \\u2013 Software to detect AI-generated contentPages displaying short descriptions of redirect targets\\nBehavior selection algorithm \\u2013 Algorithm that selects actions for intelligent agents\\nBusiness process automation \\u2013 Automation of business processes\\nCase-based reasoning \\u2013 Process of solving new problems based on the solutions of similar past problems\\nComputational intelligence \\u2013 Ability of a computer to learn a specific task from data or experimental observation\\nDigital immortality \\u2013 Hypothetical concept of storing a personality in digital form\\nEmergent algorithm \\u2013 Algorithm exhibiting emergent behavior\\nFemale gendering of AI technologies \\u2013 Gender biases in digital technologyPages displaying short descriptions of redirect targets\\nGlossary of artificial intelligence \\u2013 List of definitions of terms and concepts commonly used in the study of artificial intelligence\\nIntelligence amplification \\u2013 Use of information technology to augment human intelligence\\nMind uploading \\u2013 Hypothetical process of digitally emulating a brain\\nRobotic process automation \\u2013 Form of business process automation technology\\nWeak artificial intelligence \\u2013 Form of artificial intelligence\\nWetware computer \\u2013 Computer composed of organic material\\n\\nExplanatory notes\\nReferences\\nAI textbooks\\nThe two most widely used textbooks in 2023 (see the Open Syllabus):\\n\\nRussell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474.\\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0070087705.\\nThese were the four of the most widely used AI textbooks in 2008:\\n\\nHistory of AI\\nOther sources\\nFurther reading\\nExternal links\\n\\n\\\"Artificial Intelligence\\\". Internet Encyclopedia of Philosophy.\\nThomason, Richmond. \\\"Logic and Artificial Intelligence\\\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).\",\n          \"The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College in the U.S. during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.\\nEventually, it became obvious that researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from the U.S. Congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.  The difficult years that followed would later be known as an \\\"AI winter\\\". AI was criticized in the press and avoided by industry until the mid-2000s, but research and funding continued to grow under other names.  \\nIn the 1990s and early 2000s machine learning was applied to many problems in academia and industry. The success was due to the availability powerful computer hardware, the collection of immense data sets and the application of solid mathematical methods. In 2012, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications. Investment in AI boomed in the 2020s.\\n\\nPrecursors\\nMythical, fictional, and speculative precursors\\nMyth and legend\\nIn Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.\\nPygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.\\n\\nMedieval legends of artificial beings\\nIn Of the Nature of Things, written by the Swiss alchemist Paracelsus, he describes a procedure that he claims can fabricate an \\\"artificial man\\\". By placing the \\\"sperm of a man\\\" in horse dung, and feeding it the \\\"Arcanum of Mans blood\\\" after 40 days, the concoction will become a living infant.\\nThe earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God\\u2019s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak.\\nTakwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.\\nIn Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.\\n\\nModern fiction\\nBy the 19th century, ideas about artificial men and thinking machines were developed in fiction, as in Mary Shelley's Frankenstein  or Karel \\u010capek's R.U.R. (Rossum's Universal Robots),\\nand speculation, such as Samuel Butler's \\\"Darwin among the Machines\\\", and in real-world instances, including Edgar Allan Poe's \\\"Maelzel's Chess Player\\\". AI is a common topic in science fiction through the present.\\n\\nAutomata\\nRealistic humanoid automata were built by craftsman from many civilizations, including Yan Shi, Hero of Alexandria, Al-Jazari, Haroun al-Rashid,  Jacques de Vaucanson, Leonardo Torres y Quevedo, Pierre Jaquet-Droz and Wolfgang von Kempelen.\\nThe oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion\\u2014Hermes Trismegistus wrote that \\\"by discovering the true nature of the gods, man has been able to reproduce it\\\". English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.\\nDuring the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of M\\u00edmir. According to legend, M\\u00edmir was known for his intellect and wisdom, and was beheaded in the \\u00c6sir-Vanir War. Odin is said to have \\\"embalmed\\\" the head with herbs and spoke incantations over it such that M\\u00edmir\\u2019s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.\\n\\nFormal reasoning\\nArtificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical\\u2014or \\\"formal\\\"\\u2014reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khw\\u0101rizm\\u012b (who developed algebra and gave his name to \\\"algorithm\\\") and European scholastic philosophers such as William of Ockham and Duns Scotus.\\nSpanish philosopher Ramon Llull (1232\\u20131315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.\\n\\nIn the 17th century, Leibniz, Thomas Hobbes and Ren\\u00e9 Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: \\\"reason is nothing but reckoning\\\". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \\\"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\\\" These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.\\nThe study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift. Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \\\"can all of mathematical reasoning be formalized?\\\" His question was answered by G\\u00f6del's incompleteness proof, Turing's machine and Church's Lambda calculus.\\n\\nTheir answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction. The key insight was the Turing machine\\u2014a simple theoretical construct that captured the essence of abstract symbol manipulation.\\nThis invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.\\n\\nComputer science\\nCalculating machines were designed or built in antiquity and throughout history by many people, including \\nGottfried Leibniz,\\nJoseph Marie Jacquard, \\nCharles Babbage,\\nPercy Ludgate,\\nLeonardo Torres Quevedo,\\nVannevar Bush,\\nand others. Ada Lovelace speculated that Babbage's machine was \\\"a thinking or ... reasoning machine\\\", but warned \\\"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\\\" of the machine.\\nThe first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's and ABC and ENIAC at the University of Pennsylvania). ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann, and proved to be the most influential.\\n\\nBirth of artificial intelligence (1941-56)\\nThe earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \\\"electronic brain\\\".\\nIn the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research. Alan Turing was among the first people to seriously investigate the theoretical possibility of \\\"machine intelligence\\\". The field of \\\"artificial intelligence research\\\" was founded as an academic discipline in 1956.\\n\\nTuring Test\\nIn 1950 Turing published a landmark paper \\\"Computing Machinery and Intelligence\\\", in which he speculated about the possibility of creating machines that think.\\nIn the paper, he noted that \\\"thinking\\\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \\\"thinking\\\". This simplified version of the problem allowed Turing to argue convincingly that a \\\"thinking machine\\\" was at least plausible and the paper answered all the most common objections to the proposition. The Turing Test was the first serious proposal in the philosophy of artificial intelligence.\\n\\nArtificial neural networks\\nWalter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network. The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function. One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC. Minsky would later become one of the most important leaders and innovators in AI.\\n\\nCybernetic robots\\nExperimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.\\n\\nGame AI\\nIn 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. Arthur Samuel's checkers program, the subject of his 1959 paper \\\"Some Studies in Machine Learning Using the Game of Checkers\\\", eventually achieved sufficient skill to challenge a respectable amateur. Game AI would continue to be used as a measure of progress in AI throughout its history.\\n\\nSymbolic reasoning and the Logic Theorist\\nWhen access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.\\nIn 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \\\"Logic Theorist\\\", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some. Simon said that they had \\\"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\\\" This was an early statement of the philosophical position John Searle would later call \\\"Strong AI\\\": that machines can contain minds just as human bodies do. The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.\\n\\nDartmouth Workshop\\nThe Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline. It was organized by Marvin Minsky, John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that \\\"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\\\". The term \\\"Artificial Intelligence\\\" was introduced by John McCarthy at the workshop. The participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research. At the workshop Newell and Simon debuted the \\\"Logic Theorist\\\". The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.\\n\\nCognitive revolution\\nIn the fall of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \\\"The Magical Number Seven, Plus or Minus Two\\\". Miller wrote \\\"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\\\"\\nThis meeting was the beginning of the \\\"cognitive revolution\\\"\\u2014an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.\\nThe cognitive approach allowed researchers to consider \\\"mental objects\\\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as \\\"unobservable\\\" by earlier paradigms such as behaviorism. Symbolic mental objects would become the major focus of AI research and funding for the next several decades.\\n\\nEarly successes (1956-1974)\\nThe programs developed in the years after the Dartmouth Workshop were, to most people, simply \\\"astonishing\\\": computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \\\"intelligent\\\" behavior by machines was possible at all. Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years. Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \\\"ARPA\\\") poured money into the field. Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.\\n\\nApproaches\\nThere were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:\\n\\nReasoning as search\\nMany early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end.\\nThe principal difficulty was that, for many problems, the number of possible paths through the \\\"maze\\\" was astronomical (a situation known as a \\\"combinatorial explosion\\\"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.\\nNewell and Simon tried to capture a general version of this algorithm in a program called the \\\"General Problem Solver\\\". Other \\\"searching\\\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958) and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961. Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.\\n\\nNatural language\\nAn important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.\\nA semantic net represents concepts (e.g. \\\"house\\\", \\\"door\\\") as nodes, and relations among concepts as links between the nodes (e.g. \\\"has-a\\\"). The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.\\nJoseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot.\\n\\nMicro-worlds\\nIn the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds. They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \\\"blocks world,\\\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.\\nThis paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \\\"constraint propagation\\\"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.\\n\\nPerceptrons\\nIn the 1960s the cognitive revolution was highly influential and most funding was directed towards laboratories researching symbolic AI. However, there was one exception: the perceptron, a single-layer neural network introduced in 1958 by Frank Rosenblatt (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science). Like most AI researchers, he was optimistic about their power, predicting that a perceptron \\u201cmay eventually be able to learn, make decisions, and translate languages.\\\"\\nRosenblatt was primarily funded by Office of Naval Research. Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights. A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II had 6600 adjustable weights, and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets.\\nMost of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers. The hardware diversity was particularly clear in the different technologies used in implementing the adjustable weights. The perceptron machines and the SNARC used potentiometers moved by electric motors. ADALINE used memistors adjusted by electroplating, though they also used simulations on an IBM 1620 computer. The MINOS machines used ferrite cores with multiple holes in them that could be individually blocked, with the degree of blockage representing the weights.\\nHowever, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s. In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years. The competition for government funding ended with the victory of symbolic AI approaches over neural networks. \\nMinsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics.  \\nThe main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers). Rosenblatt attempted to gather funds for building larger perceptron machines, but he died in a boating accident in 1971.\\n\\nOptimism\\nThe first generation of AI researchers made these predictions about their work:\\n\\n1958, H. A. Simon and Allen Newell: \\\"within ten years a digital computer will be the world's chess champion\\\" and \\\"within ten years a digital computer will discover and prove an important new mathematical theorem.\\\"\\n1965, H. A. Simon: \\\"machines will be capable, within twenty years, of doing any work a man can do.\\\"\\n1967, Marvin Minsky: \\\"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.\\\"\\n1970, Marvin Minsky (in Life Magazine): \\\"In from three to eight years we will have a machine with the general intelligence of an average human being.\\\"\\n\\nFinancing\\nIn June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the \\\"AI Group\\\" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s. DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963. Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965. These four institutions would continue to be the main centers of AI research and funding in academia for many years.\\nThe money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \\\"fund people, not projects!\\\" and allowed researchers to pursue whatever directions might interest them. This created a freewheeling atmosphere at MIT that gave birth to the hacker culture, but this \\\"hands off\\\" approach did not last.\\n\\nFirst AI Winter (1974\\u20131980)\\nIn the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced. The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.\\nThese setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories and the critiques were largely ignored. General public interest in the field continued to grow, the number of researchers increased dramatically, and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argues that there was no winter, and AI researcher Nils Nilsson described this period as the most \\\"exciting\\\" time to work in AI.\\n\\nProblems\\nIn the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \\\"toys\\\". AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:\\n\\nLimited computer power. There was not enough memory or processing speed to accomplish anything truly useful. For example: Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only 20 words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. \\\"With enough horsepower,\\\" he wrote, \\\"anything will fly\\\".\\nIntractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can only be solved in exponential time. Finding optimal solutions to these problems requires extraordinary amounts of computer time, except when the problems are trivial. This meant that many of the \\\"toy\\\" solutions used by AI would never scale to useful systems.\\nCommonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a vast amount of information. No one in 1970 could build a database large enough and no one knew how a program might learn so much information.\\nMoravec's paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult, and research into vision and robotics made little progress in the early 1970s.\\nThe frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.\\n\\nDecrease in funding\\nThe agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support. In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \\\"grandiose objectives\\\" and led to the dismantling of AI research in that country. (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.) DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.\\nHans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \\\"Many researchers were caught up in a web of increasing exaggeration.\\\"\\n However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \\\"mission-oriented direct research, rather than basic undirected research\\\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.\\n\\nThe major laboratories (MIT, Stanford and CMU) had been receiving generous support from the U.S. military, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.\\n\\nPhilosophical and ethical critiques\\nSeveral philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that G\\u00f6del's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \\\"symbol processing\\\" and a great deal of embodied, instinctive, unconscious \\\"know how\\\". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \\\"understand\\\" the symbols that it uses (a quality called \\\"intentionality\\\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \\\"thinking\\\".\\nThese critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \\\"know how\\\" or \\\"intentionality\\\" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle \\\"they misunderstand, and should be ignored.\\\" Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \\\"dared not be seen having lunch with me.\\\" Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \\\"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\\\" and was unprofessional and childish.\\nWeizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \\\"computer program which can conduct psychotherapeutic dialogue\\\" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.\\n\\nLogic at Stanford, CMU and Edinburgh\\nLogic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal.\\nIn 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog.\\nProlog uses a subset of logic (Horn clauses, closely related to \\\"rules\\\" and \\\"production rules\\\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.\\nCritics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.\\nMcCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems\\u2014not machines that think as people do.\\n\\nMIT's \\\"anti-logic\\\" approach\\nAmong the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \\\"story understanding\\\" and \\\"object recognition\\\" that required a machine to think like a person. In order to use ordinary concepts like \\\"chair\\\" or \\\"restaurant\\\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that \\\"using precise language to describe essentially imprecise concepts doesn't make them any more precise.\\\" Schank described their \\\"anti-logic\\\" approaches as \\\"scruffy\\\", as opposed to the \\\"neat\\\" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.\\nIn 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be \\\"logical\\\", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures \\\"frames\\\". Schank used a version of frames he called \\\"scripts\\\" to successfully answer questions about short stories in English.\\n\\nThe emergence of non-monotonic logics\\nThe logicians rose to the challenge. Pat Hayes claimed that \\\"most of 'frames' is just a new syntax for parts of first-order\\nlogic.\\\" But he noted that \\\"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\\\". In the meanwhile, Ray Reiter admitted that \\\"conventional logics, such as first-order\\nlogic, lack the expressive power to adequately represent the knowledge required for reasoning by default\\\". He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \\\"procedural equivalent\\\" as negation as failure in Prolog.\\nThe closed world assumption, as formulated by Reiter, \\\"is not a first-order notion. (It is a meta notion.)\\\" However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.\\nDuring the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.\\n\\nBoom (1980\\u20131987)\\nIn the 1980s, a form of AI program called \\\"expert systems\\\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. \\nAlthough symbolic knowledge representation and logical reasoning produced these useful applications in the 80s, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as connectionism, robotics, and soft computing.\\n\\nExpert systems become widely used\\nAn expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts. The earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings. MYCIN, developed in 1972, diagnosed infectious blood diseases. They demonstrated the feasibility of the approach.\\nExpert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem) and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.\\nIn 1980, an expert system called XCON was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986. Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments. An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.\\n\\nGovernment funding increases\\nIn 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings. Much to the chagrin of scruffies, they chose Prolog as the primary computer language for the project.\\nOther countries responded with new programs of their own. The UK began the \\u00a3350 million Alvey project. A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \\\"MCC\\\") to fund large scale projects in AI and information technology. DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.\\n\\nKnowledge revolution\\nThe power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \\\"AI researchers were beginning to suspect\\u2014reluctantly, for it violated the scientific canon of parsimony\\u2014that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\\\" writes Pamela McCorduck. \\\"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\\\". Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.\\nThe 1980s also saw the birth of Cyc, the first attempt to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started and led the project, argued that there is no shortcut \\u2015 the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand. The project was not expected to be completed for many decades.\\nChess playing programs HiTech and Deep Thought defeated chess masters in 1989. Both were developed by Carnegie Mellon University; Deep Thought development paved the way for Deep Blue.\\n\\nRevival of neural networks: connectionism\\nIn 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \\\"Hopfield net\\\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically. Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \\\"backpropagation\\\". These two discoveries helped to revive the exploration of artificial neural networks.\\nNeural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened connectionism and there was a considerable debate between advocates of symbolic AI the \\\"connectionists\\\".\\nIn 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.\\n\\nRobotics and embodied reason\\nRodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body \\u2014 it needs to perceive, move, survive and deal with the world. Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \\\"from the bottom up\\\".\\nA precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)\\nIn his 1990 paper \\\"Elephants Don't Play Chess,\\\" robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \\\"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\\\"\\nIn the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the embodied mind thesis.\\n\\nBust: second AI winter\\nThe business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable. The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \\\"artificial intelligence\\\".\\nOver the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability. By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been.\\n\\nAI winter\\nThe term \\\"AI winter\\\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow. Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.\\nThe first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.\\nEventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, they were \\\"brittle\\\" (i.e., they could make grotesque mistakes when given unusual inputs), and they fell prey to problems (such as the qualification problem) that had been identified years earlier. Expert systems proved useful, but only in a few special contexts.\\nIn the late 1980s, the Strategic Computing Initiative cut funding to AI \\\"deeply and brutally\\\". New leadership at DARPA had decided that AI was not \\\"the next wave\\\" and directed funds towards projects that seemed more likely to produce immediate results.\\nBy 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like \\\"carry on a casual conversation\\\" had not been met by 2010. As with other AI projects, expectations had run much higher than what was actually possible.\\nOver 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI. In 1994, HP Newquist stated in The Brain Makers that \\\"The immediate future of artificial intelligence\\u2014in its commercial form\\u2014seems to rest in part on the continued success of neural networks.\\\"\\n\\nAI behind the scenes\\nIn the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems\\nand their solutions proved to be useful throughout the technology industry, such as\\ndata mining, industrial robotics, logistics,\\nspeech recognition,\\nbanking software,\\nmedical diagnosis\\nand Google's search engine.\\nThe field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains \\\"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\\\"\\nMany researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \\\"cognitive systems\\\" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \\\"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\\\"\\n\\nMilestones and Moore's law\\nOn May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second.\\nIn 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws. In February 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.\\nThese successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s. In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal\\u2013oxide\\u2013semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of \\\"raw computer power\\\" was slowly being overcome.\\n\\nIntelligent agents\\nA new paradigm called \\\"intelligent agents\\\" became widely accepted during the 1990s. Although earlier researchers had proposed modular \\\"divide and conquer\\\" approaches to AI, the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI. When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.\\nAn intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \\\"intelligent agents\\\", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as \\\"the study of intelligent agents\\\". This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.\\nThe paradigm gave researchers license to study isolated problems and find solutions that were both verifiable and useful. It provided a common language to describe problems and share their solutions with each other, and with other fields that also used concepts of abstract agents, like economics and control theory. It was hoped that a complete agent architecture (like Newell's SOAR) would one day allow researchers to build more versatile and intelligent systems out of interacting intelligent agents.\\n\\nProbabilistic reasoning and greater rigor\\nAI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past. There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \\\"scientific\\\" discipline.\\nJudea Pearl's influential 1988 book brought probability and decision theory into AI. Among the many new tools in use were Bayesian networks, hidden Markov models, information theory, stochastic modeling and classical optimization. Precise mathematical descriptions were also developed for \\\"computational intelligence\\\" paradigms like neural networks and evolutionary algorithms.\\n\\nDeep learning, big data (2011\\u20132020)\\nIn the first decades of the 21st century, access to large amounts of data (known as \\\"big data\\\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper \\\"Big data: The next frontier for innovation, competition, and productivity\\\" that \\\"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\\\".\\nBy 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \\\"frenzy\\\". The applications of big data began to reach into other fields as well, such as training models in ecology and for various applications in economics. Advances in deep learning (particularly deep convolutional neural networks and recurrent neural networks) drove progress and research in image and video processing, text analysis, and even speech recognition.\\nThe first global AI Safety Summit was held in Bletchley Park, UK, in November 2023 to discuss the short- and long-term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. Twenty-eight countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.\\n\\nDeep learning\\nDeep learning is a branch of machine learning that models high level abstractions in data by using a deep graph with many processing layers. According to the Universal approximation theorem, deep-ness isn't necessary for a neural network to be able to approximate arbitrary continuous functions. Even so, there are many problems that are common to shallow networks (such as overfitting) that deep networks help avoid. As such, deep neural networks are able to realistically generate much more complex models as compared to their shallow counterparts.\\nHowever, deep learning has problems of its own. A common problem for recurrent neural networks is the vanishing gradient problem, which is where gradients passed between layers gradually shrink and literally disappear as they are rounded off to zero. There have been many methods developed to approach this problem, such as Long short-term memory units.\\nState-of-the-art deep neural network architectures can sometimes even rival human accuracy in fields like computer vision, specifically on things like the Modified National Institute of Standards and Technology (MNIST) database, and traffic sign recognition.\\nLanguage processing engines powered by smart search engines can easily beat humans at answering general trivia questions (such as IBM Watson), and recent developments in deep learning have produced surprising results in competing with humans, specifically in games like Go, and Doom (which, being a first-person shooter game, has sparked some controversy).\\n\\nBig data\\nBig data refers to a collection of data that cannot be captured, managed, and processed by conventional software tools within a certain time frame. It is a massive amount of decision-making, insight, and process optimization capabilities that require new processing models. In the book \\\"Big Data Era\\\" by Victor Meyer Schonberg and Kenneth Cooke, big data means that instead of random analysis (sample survey), all data is used for analysis. The \\\"5V\\\" characteristics of big data, as proposed by IBM, are: Volume, Velocity, Variety, Value, Veracity.\\nThe strategic significance of big data technology is not to master huge data information, but to specialize in meaningful data. In other words, if big data is likened to an industry, the key to realizing profitability in this industry is to increase the \\\"process capability\\\" of the data and realize the \\\"value added\\\" of the data through \\\"processing\\\".\\n\\nLarge language models, AI boom (2020\\u2013present)\\nThe AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of reasoning, cognition, attention and creativity. The new AI era has been said to have begun around 2022\\u20132023, with the public release of scaled large language models (LLMs) such as ChatGPT.\\n\\nLarge language models\\nIn 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and later became widely used in large language models.\\nFoundation models, which are large language models trained on vast quantities of unlabeled data that can be adapted to a wide range of downstream tasks, began to be developed in 2018.\\nModels such as GPT-3 released by OpenAI in 2020, and Gato released by DeepMind in 2022, have been described as important achievements of machine learning.\\nIn 2023, Microsoft Research tested the GPT-4 large language model with a large variety of tasks, and concluded that \\\"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\\\".\\n\\nSee also\\nHistory of artificial neural networks\\nHistory of knowledge representation and reasoning\\nHistory of natural language processing\\nOutline of artificial intelligence\\nProgress in artificial intelligence\\nTimeline of artificial intelligence\\nTimeline of machine learning\\n\\nNotes\\n\\n\\n== References ==\",\n          \"The Data Science Institute is a research institute at the Imperial College London founded in May 2014. The institute is one of five Global Institutes at Imperial College London, alongside the Institute of Global Health Innovation, Energy Futures Lab, Institute for Security Science and Technology, and the Grantham Institute - Climate Change and Environment.\\nThe Data Science Institute has partnerships with international industry and academia, with formal investments from Chinese multinational telecoms company Huawei, multinational consultancy KPMG, and Zhejiang University, China.\\nThe goal of the institute is to enhance multidisciplinary data science research across the whole of Imperial College by coordinating and promoting data-driven research and education activities. These activities cover all areas across the College including engineering, medicine, natural sciences, and business.\\nThe institute houses a custom built large-scale immersive data visualization facility called the KPMG Data Observatory, which has a resolution of 132 megapixels that is thought to be the largest such system in Europe.\\n\\nReferences\\nExternal links\\nOfficial website\\nhttps://www.imperial.ac.uk/research-and-innovation/about-imperial-research/global-institutes/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"ai ai broad sense intelligence exhibit machine particularly computer system field research computer science develop study method software enable machine perceive environment use learn intelligence take action maximize chance achieve define goal machine may call ai high profile application ai include advance web search engine eg google search recommendation system use youtube amazon netflix interact via human speech eg google assistant siri alexa autonomous vehicle eg waymo generative creative tool eg chatgpt apple intelligence ai art superhuman play analysis strategy game eg chess go however many ai application perceive ai lot cut edge ai filter general application often without call ai something become useful enough common enough label ai anymore alan turing first person conduct substantial research field call machine intelligence ai found academic discipline 1956 consider found father ai john mccarthy marvin minksy nathaniel rochester claude shannon field go multiple cycle optimism follow period disappointment loss fund know ai winter fund interest vastly increase 2012 deep learn surpass previous ai technique 2017 transformer architecture lead ai boom early 2020 company university laboratory overwhelmingly base unite state pioneer significant advance ai grow use ai 21t century influence societal economic shift towards increase automation data drive decision make integration ai system various economic sector area life impact job market healthcare government industry education propaganda disinformation raise question long term effect ethical implication risk ai prompt discussion regulatory policy ensure safety benefit technology various subfields ai research center around particular goal use particular tool traditional goal ai research include reason knowledge representation plan learn natural language process perception support robotics general intelligence ability complete task performable human least equal level among field long term goal reach goal ai researcher adapt integrate wide range technique include search mathematical optimization formal logic artificial neural network method base statistic operation research economics ai also draw upon psychology linguistics philosophy neuroscience field goal general problem simulate create intelligence break subproblems consist particular trait capability researcher expect intelligent system display trait describe receive attention cover scope ai research reason problem solve early researcher develop algorithm imitate step step reason human use solve puzzle make logical deduction late 1980 1990 method develop deal uncertain incomplete information employ concept probability economics many algorithm insufficient solve large reason problem experience combinatorial explosion become exponentially slow problem grow even human rarely use step step deduction early ai research could model solve problem use fast intuitive judgment accurate efficient reason unsolved problem knowledge representation knowledge representation knowledge engineer allow ai program answer question intelligently make deduction real world fact formal knowledge representation use content base index retrieval scene interpretation clinical decision support knowledge discovery mine interest actionable inference large database area knowledge base body knowledge represent form use program ontology set object relation concept property use particular domain knowledge knowledge base need represent thing object property category relation object situation event state time cause effect knowledge knowledge know people know default reason thing human assume true tell differently remain true even fact change many aspect domain knowledge among difficult problem knowledge representation breadth commonsense knowledge set atomic fact average person know enormous sub symbolic form commonsense knowledge much people know represent fact statement could express verbally also difficulty knowledge acquisition problem obtain knowledge ai application plan decision make agent anything perceive take action world rational agent goal preference take action make happen automate plan agent specific goal automate decision make agent preference situation would prefer situation try avoid decision make agent assign number situation call utility measure much agent prefer possible action calculate expect utility utility possible outcome action weight probability outcome occur choose action maximum expect utility classical plan agent know exactly effect action real world problem however agent may certain situation unknown unobservable may know certain happen possible action deterministic must choose action make probabilistic guess reassess situation see action work problem agent preference may uncertain especially agent human involve learn eg inverse reinforcement learn agent seek information improve preference information value theory use weigh value exploratory experimental action space possible future action situation typically intractably large agent must take action evaluate situation uncertain outcome markov decision process transition model describe probability particular action change state particular way reward function supply utility state cost action policy associate decision possible state policy could calculate eg iteration heuristic learn game theory describe rational behavior multiple interact agent use ai program make decision involve agent learn machine learn study program improve performance give task automatically part ai begin several kind machine learn unsupervised learn analyze stream data find pattern make prediction without guidance supervise learn require human label input data first come two main variety classification program must learn predict category input belong regression program must deduce numeric function base numeric input reinforcement learn agent reward good response punish bad one agent learn choose response classify good transfer learn knowledge gain one problem apply new problem deep learn type machine learn run input biologically inspire artificial neural network type learn computational learn theory ass learner computational complexity sample complexity much data require notion optimization natural language process natural language process nlp allow program read write communicate human language english specific problem include speech recognition speech synthesis machine translation information extraction information retrieval question answer early work base noam chomsky generative grammar semantic network difficulty word sense disambiguation unless restrict small domain call micro world due common sense knowledge problem margaret masterman believe mean grammar key understand language thesaurus dictionary basis computational language structure modern deep learn technique nlp include word embed represent word typically vector encode mean transformer deep learn architecture use attention mechanism others 2019 generative pre train transformer gpt language model begin generate coherent text 2023 model able get human level score bar exam sit test gre test many real world application perception machine perception ability use input sensor camera microphone wireless signal active lidar sonar radar tactile sensor deduce aspect world computer vision ability analyze visual input field include speech recognition image classification facial recognition object recognitionobject track robotic perception social intelligence affective compute interdisciplinary umbrella comprise system recognize interpret process simulate human feel emotion mood example virtual assistant program speak conversationally even banter humorously make appear sensitive emotional dynamic human interaction otherwise facilitate humancomputer interaction however tend give naive user unrealistic conception intelligence exist computer agent moderate success relate affective compute include textual sentiment analysis recently multimodal sentiment analysis wherein ai classify affect display videotape subject general intelligence machine artificial general intelligence able solve wide variety problem breadth versatility similar human intelligence technique ai research use wide variety technique accomplish goal search optimization ai solve many problem intelligently search many possible solution two different kind search use ai state space search local search state space search state space search search tree possible state try find goal state example plan algorithm search tree goal subgoals attempt find path target goal process call mean end analysis simple exhaustive search rarely sufficient real world problem search space number place search quickly grow astronomical number result search slow never complete heuristic rule thumb help prioritize choice likely reach goal adversarial search use game play program chess go search tree possible move counter move look win position local search local search use mathematical optimization find solution problem begin form guess refine incrementally gradient descent type local search optimize set numerical parameter incrementally adjust minimize loss function variant gradient descent commonly use train neural network another type local search evolutionary computation aim iteratively improve set candidate solution mutate recombine select fit survive generation distribute search process coordinate via swarm intelligence algorithm two popular swarm algorithm use search particle swarm optimization inspire bird flock ant colony optimization inspire ant trail logic formal logic use reason knowledge representation formal logic come two main form propositional logic operate statement true false use logical connective imply predicate logic also operate object predicate relation use quantifier every x x y deductive reason logic process prove new statement conclusion statement give assume true premise proof structure proof tree nod label sentence child nod connect parent nod inference rule give problem set premise problem solve reduce search proof tree whose root node label solution problem whose leaf nod label premise axiom case horn clause problem solve search perform reason forward premise backwards problem general case clausal form first order logic resolution single axiom free rule inference problem solve prove contradiction premise include negation problem solve inference horn clause logic first order logic undecidable therefore intractable however backward reason horn clause underpin computation logic program language prolog turing complete moreover efficiency competitive computation symbolic program language fuzzy logic assign degree truth 0 1 therefore handle proposition vague partially true non monotonic logic include logic program negation failure design handle default reason specialize version logic develop describe many complex domain probabilistic method uncertain reason many problem ai include reason plan learn perception robotics require agent operate incomplete uncertain information ai researcher devise number tool solve problem use method probability theory economics precise mathematical tool develop analyze agent make choice plan use decision theory decision analysis information value theory tool include model markov decision process dynamic decision network game theory mechanism design bayesian network tool use reason use bayesian inference algorithm learn use expectationmaximization algorithm plan use decision network perception use dynamic bayesian network probabilistic algorithm also use filter prediction smooth find explanation stream data thus help perception system analyze process occur time eg hide markov model kalman filter classifier statistical learn method simple ai application divide two type classifier eg shiny diamond one hand controller eg diamond pick hand classifier function use pattern match determine close match fine tune base choose example use supervise learn pattern also call observation label certain predefined class observation combine class label know data set new observation receive observation classify base previous experience many kind classifier use decision tree simple widely use symbolic machine learn algorithm k near neighbor algorithm widely use analogical ai mid 1990 kernel method support vector machine svm displace k near neighbor 1990 naive bay classifier reportedly widely use learner google due part scalability neural network also use classifier artificial neural network artificial neural network base collection nod also know artificial neuron loosely model neuron biological brain train recognise pattern train recognise pattern fresh data input least one hide layer nod output node apply function weight cross specify threshold data transmit next layer network typically call deep neural network least 2 hide layer learn algorithm neural network use local search choose weight get right output input train common train technique backpropagation algorithm neural network learn model complex relationship input output find pattern data theory neural network learn function feedforward neural network signal pas one direction recurrent neural network fee output signal back input allow short term memory previous input event long short term memory successful network architecture recurrent network perceptrons use single layer neuron deep learn use multiple layer convolutional neural network strengthen connection neuron close especially important image process local set neuron must identify edge network identify object deep learn deep learn use several layer neuron network input output multiple layer progressively extract high level feature raw input example image process low layer may identify edge high layer may identify concept relevant human digit letter face deep learn profoundly improve performance program many important subfields ai include computer vision speech recognition natural language process image classification others reason deep learn perform well many application know 2023 sudden success deep learn 20122015 occur new discovery theoretical breakthrough deep neural network backpropagation describe many people far back 1950 two factor incredible increase computer power include hundred fold increase speed switch gpus availability vast amount train data especially giant curated datasets use benchmark test imagenet gpt generative pre train transformer gpt large language model llm base semantic relationship word sentence natural language process text base gpt model pretrained large corpus text internet pretraining consist predict next token token usually word subword punctuation throughout pretraining gpt model accumulate knowledge world generate human like text repeatedly predict next token typically subsequent train phase make model truthful useful harmless usually technique call reinforcement learn human feedback rlhf current gpt model prone generate falsehood call hallucination although reduce rlhf quality data use chatbots allow people ask question request task simple text current model service include gemini formerly bard chatgpt grok claude copilot llama multimodal gpt model process different type data modality image video sound text specialize hardware software late 2010 graphic process unit gpus increasingly design ai specific enhancement use specialize tensorflow software replace previously use central process unit cpu dominant mean large scale commercial academic machine learn model train specialize program language prolog use early ai research general purpose program language like python become predominant application ai machine learn technology use essential application 2020 include search engine google search target online advertisement recommendation system offer netflix youtube amazon drive internet traffic target advertise adsense facebook virtual assistant siri alexa autonomous vehicle include drone ada self drive car automatic language translation microsoft translator google translate facial recognition apple face id microsoft deepface google facenet image label use facebook apple iphoto tiktok deployment ai may oversee chief automation officer cao health medicine application ai medicine medical research potential increase patient care quality life lens hippocratic oath medical professional ethically compel use ai application accurately diagnose treat patient medical research ai important tool process integrate big data particularly important organoid tissue engineer development use microscopy image key technique fabrication suggest ai overcome discrepancy fund allocate different field research new ai tool deepen understand biomedically relevant pathway example alphafold 2 2021 demonstrate ability approximate hour rather month 3d structure protein 2023 report ai guide drug discovery help find class antibiotic capable kill two different type drug resistant bacteria 2024 researcher use machine learn accelerate search parkinson disease drug treatment aim identify compound block clump aggregation alpha synuclein protein characterise parkinson disease able speed initial screen process ten fold reduce cost thousand fold game game play program use since 1950 demonstrate test ai advance technique deep blue become first computer chess play system beat reign world chess champion garry kasparov 11 may 1997 2011 jeopardy quiz show exhibition match ibm question answer system watson defeat two great jeopardy champion brad rutter ken jennings significant margin march 2016 alphago 4 5 game go match go champion lee sedol become first computer go play system beat professional go player without handicap 2017 defeat ke jie best go player world program handle imperfect information game poker play program pluribus deepmind develop increasingly generalistic reinforcement learn model muzero could train play chess go atari game 2019 deepmind alphastar achieve grandmaster level starcraft ii particularly challenge real time strategy game involve incomplete knowledge happen map 2021 ai agent compete playstation gran turismo competition win four world best gran turismo driver use deep reinforcement learn 2024 google deepmind introduce sima type ai capable autonomously play nine previously unseen open world video game observe screen output well execute short specific task response natural language instruction mathematics mathematics special form formal step step reason use contrast llm gpt 4 turbo gemini ultra claude opus llama 2 mistral large work probabilistic model produce wrong answer form hallucination therefore need large database mathematical problem learn also method supervise fine tune train classifier human annotate data improve answer new problem learn correction alternatively dedicate model mathematic problem solve high precision outcome include proof theorem develop alpha tensor alpha geometry alpha proof google deepmind llemma eleuther julius natural language use describe mathematical problem converter transform prompt formal language lean define mathematic task model develop solve challenge problem reach good result benchmark test others serve educational tool mathematics finance finance one fast grow sector apply ai tool deploy retail online bank investment advice insurance automate robot adviser use year world pension expert like nicolas firzli insist may early see emergence highly innovative ai inform financial product service deployment ai tool simply automatise thing destroy ten thousand job bank financial plan pension advice process im sure unleash new wave eg sophisticate pension innovation military various country deploy ai military application main application enhance command control communication sensor integration interoperability research target intelligence collection analysis logistics cyber operation information operation semiautonomous autonomous vehicle ai technology enable coordination sensor effector threat detection identification mark enemy position target acquisition coordination deconfliction distribute joint fire network combat vehicle involve man unman team ai incorporate military operation iraq syria november 2023 u vice president kamala harris disclose declaration sign 31 nation set guardrail military use ai commitment include use legal review ensure compliance military ai international law cautious transparent development technology generative ai early 2020 generative ai gain widespread prominence march 2023 58 u adult hear chatgpt 14 try increase realism ease use ai base text image generator midjourney dall e stable diffusion spark trend viral ai generate photo widespread attention gain fake photo pope francis wear white puffer coat fictional arrest donald trump hoax attack pentagon well usage professional creative art industry specific task also thousand successful ai application use solve specific problem specific industry institution 2017 survey one five company report incorporate ai offer process example energy storage medical diagnosis military logistics application predict result judicial decision foreign policy supply chain management ai application evacuation disaster management grow ai use investigate people evacuate large scale small scale evacuation use historical data gps video social medium ai provide real time information real time evacuation condition agriculture ai help farmer identify area need irrigation fertilization pesticide treatment increase yield agronomist use ai conduct research development ai use predict ripen time crop tomato monitor soil moisture operate agricultural robot conduct predictive analytics classify livestock pig call emotion automate greenhouse detect disease pest save water ai use astronomy analyze increase amount available data application mainly classification regression cluster forecast generation discovery development new scientific insight example discover exoplanets forecast solar activity distinguish signal instrumental effect gravitational wave astronomy could also use activity space space exploration include analysis data space mission real time science decision spacecraft space debris avoidance autonomous operation ethic ai potential benefit potential risk ai may able advance science find solution serious problem demis hassabis deep mind hop solve intelligence use solve everything else however use ai become widespread several unintended consequence risk identify production system sometimes factor ethic bias ai train process especially ai algorithm inherently unexplainable deep learn risk harm privacy copyright machine learn algorithm require large amount data technique use acquire data raise concern privacy surveillance copyright technology company collect wide range data user include online activity geolocation data video audio example order build speech recognition algorithm amazon record million private conversation allow temporary worker listen transcribe opinion widespread surveillance range see necessary evil clearly unethical violation right privacy ai developer argue way deliver valuable application develop several technique attempt preserve privacy still obtain data data aggregation de identification differential privacy since 2016 privacy expert cynthia dwork begin view privacy term fairness brian christian write expert pivot question know question theyre generative ai often train unlicensed copyright work include domain image computer code output use rationale fair use expert disagree well circumstance rationale hold court law relevant factor may include purpose character use copyright work effect upon potential market copyright work website owner wish content scrap indicate robotstxt file 2023 lead author include john grisham jonathan franzen sue ai company use work train generative ai another discus approach envision separate sui generis system protection creation generate ai ensure fair attribution compensation human author dominance tech giant commercial ai scene dominate big tech company alphabet inc amazon apple inc meta platform microsoft player already vast majority exist cloud infrastructure compute power data center allow entrench marketplace substantial power need environmental impact january 2024 international energy agency iea release electricity 2024 analysis forecast 2026 forecast electric power use first iea report make projection data center power consumption ai cryptocurrency report state power demand use might double 2026 additional electric power usage equal electricity use whole japanese nation prodigious power consumption ai responsible growth fossil fuel use might delay close obsolete carbon emit coal energy facility feverish rise construction data center throughout u make large technology firm eg microsoft meta google amazon voracious consumer electric power project electric consumption immense concern fulfil matter source chatgpt search involve use 10 time electrical energy google search large firm haste find power source nuclear energy geothermal fusion tech firm argue long view ai eventually kind environment need energy ai make power grid efficient intelligent assist growth nuclear power track overall carbon emission accord technology firm 2024 goldman sachs research paper ai data center come u power demand surge find u power demand likely experience growth see generation forecast 2030 u data center consume 8 u power oppose 3 2022 presage growth electrical power generation industry variety meansdata center need electrical power might max electrical grid big tech company counter ai use maximize utilization grid 2024 wall street journal report big ai company begin negotiation u nuclear power provider provide electricity data center march 2024 amazon purchase pennsylvania nuclear power data center 650 million u misinformation youtube facebook others use recommender system guide user content ai program give goal maximize user engagement goal keep people watch ai learn user tend choose misinformation conspiracy theory extreme partisan content keep watch ai recommend user also tend watch content subject ai lead people filter bubble receive multiple version misinformation convince many user misinformation true ultimately undermine trust institution medium government ai program correctly learn maximize goal result harmful society u election 2016 major technology company take step mitigate problem 2022 generative ai begin create image audio video text indistinguishable real photograph record film human write possible bad actor use technology create massive amount misinformation propaganda ai pioneer geoffrey hinton express concern ai enable authoritarian leader manipulate electorate large scale among risk algorithmic bias fairness statistic bias systematic error deviation correct value context fairness often refer tendency favor certain group individual characteristic usually way consider unfair harmful statistically unbiased ai system produce disparate outcome different demographic group may thus view bias ethical sense field fairness study prevent harm algorithmic bias various conflict definition mathematical model fairness notion depend ethical assumption influence belief society one broad category distributive fairness focus outcome often identify group seek compensate statistical disparity representational fairness try ensure ai system reinforce negative stereotype render certain group invisible procedural fairness focus decision process rather outcome relevant notion fairness may depend context notably type ai application stakeholder subjectivity notion bias fairness make difficult company operationalize access sensitive attribute race gender also consider many ai ethicist necessary order compensate bias may conflict anti discrimination law machine learn application bias learn bias data developer may aware bias exist bias introduce way train data select way model deploy bias algorithm use make decision seriously harm people medicine finance recruitment house police algorithm may cause discrimination june 28 2015 google photo new image label feature mistakenly identify jacky alcine friend gorilla black system train dataset contain image black people problem call sample size disparity google fix problem prevent system label anything gorilla eight year late 2023 google photo still could identify gorilla neither could similar product apple facebook microsoft amazon compas commercial program widely use u court ass likelihood defendant become recidivist 2016 julia angwin propublica discover compas exhibit racial bias despite fact program tell race defendant although error rate white black calibrate equal exactly 61 error race different system consistently overestimate chance black person would offend would underestimate chance white person would offend 2017 several researcher show mathematically impossible compas accommodate possible measure fairness base rat offense different white black data program make bias decision even data explicitly mention problematic feature race gender feature correlate feature like address shop history first name program make decision base feature would race gender moritz hardt say robust fact research area fairness blindness work criticism compas highlight machine learn model design make prediction valid assume future resemble past train data include result racist decision past machine learn model must predict racist decision make future application use prediction recommendation recommendation likely racist thus machine learn well suit help make decision area hope future good past descriptive rather prescriptive bias unfairness may go undetected developer overwhelmingly white male among ai engineer 4 black 20 woman 2022 conference fairness accountability transparency acm facct 2022 association compute machinery seoul south korea present publish find recommend ai robotics system demonstrate free bias mistake unsafe use self learn neural network train vast unregulated source flaw internet data curtail lack transparency many ai system complex designer cannot explain reach decision particularly deep neural network large amount non linear relationship input output popular explainability technique exist impossible certain program operate correctly one know exactly work many case machine learn program pas rigorous test nevertheless learn something different programmer intend example system could identify skin disease good medical professional find actually strong tendency classify image ruler cancerous picture malignancy typically include ruler show scale another machine learn system design help effectively allocate medical resource find classify patient asthma low risk die pneumonia asthma actually severe risk factor since patient asthma would usually get much medical care relatively unlikely die accord train data correlation asthma low risk die pneumonia real mislead people harm algorithm decision right explanation doctor example expect clearly completely explain colleague reason behind decision make early draft european union general data protection regulation 2016 include explicit statement right exist industry expert note unsolved problem solution sight regulator argue nevertheless harm real problem solution tool use darpa establish xai explainable ai program 2014 try solve problem several approach aim address transparency problem shap enable visualise contribution feature output lime locally approximate model output simple interpretable model multitask learn provide large number output addition target classification output help developer deduce network learn deconvolution deepdream generative method allow developer see different layer deep network computer vision learn produce output suggest network learn generative pre train transformer anthropic develop technique base dictionary learn associate pattern neuron activation human understandable concept bad actor weaponize ai ai provide number tool useful bad actor authoritarian government terrorist criminal rogue state lethal autonomous weapon machine locate select engage human target without human supervision widely available ai tool use bad actor develop inexpensive autonomous weapon produce scale potentially weapon mass destruction even use conventional warfare unlikely unable reliably choose target could potentially kill innocent person 2014 30 nation include china support ban autonomous weapon unite nation convention certain conventional weapon however unite state others disagree 2015 fifty country report research battlefield robot ai tool make easy authoritarian government efficiently control citizen several way face voice recognition allow widespread surveillance machine learn operate data classify potential enemy state prevent hide recommendation system precisely target propaganda misinformation maximum effect deepfakes generative ai aid produce misinformation advance ai make authoritarian centralize decision make competitive liberal decentralize system market low cost difficulty digital warfare advance spyware technology available since 2020 early ai facial recognition system already use mass surveillance china many way ai expect help bad actor foresee example machine learn ai able design ten thousand toxic molecule matter hour technological unemployment economist frequently highlight risk redundancy ai speculate unemployment adequate social policy full employment past technology tend increase rather reduce total employment economist acknowledge uncharted territory ai survey economist show disagreement whether increase use robot ai cause substantial increase long term unemployment generally agree could net benefit productivity gain redistribute risk estimate vary example 2010 michael osborne carl benedikt frey estimate 47 u job high risk potential automation oecd report classify 9 u job high risk methodology speculate future employment level criticise lack evidential foundation imply technology rather social policy create unemployment oppose redundancy april 2023 report 70 job chinese video game illustrator eliminate generative ai unlike previous wave automation many middle class job may eliminate ai economist state 2015 worry ai could white collar job steam power blue collar one industrial revolution worth take seriously job extreme risk range paralegal fast food cook job demand likely increase care relate profession range personal healthcare clergy early day development ai argument example put forward joseph weizenbaum whether task do computer actually do give difference computer human quantitative calculation qualitative value base judgement existential risk argue ai become powerful humanity may irreversibly lose control could physicist stephen hawk state spell end human race scenario common science fiction computer robot suddenly develop human like self awareness sentience consciousness become malevolent character sci fi scenario mislead several way first ai require human like sentience existential risk modern ai program give specific goal use learn intelligence achieve philosopher nick bostrom argue one give almost goal sufficiently powerful ai may choose destroy humanity achieve use example paperclip factory manager stuart russell give example household robot try find way kill owner prevent unplug reason cant fetch coffee dead order safe humanity superintelligence would genuinely align humanity morality value fundamentally side second yuval noah harari argue ai require robot body physical control pose existential risk essential part civilization physical thing like ideology law government money economy make language exist story billion people believe current prevalence misinformation suggest ai could use language convince people believe anything even take action destructive opinion amongst expert industry insider mix sizable fraction concern unconcerned risk eventual superintelligent ai personality stephen hawk bill gate elon musk well ai pioneer yoshua bengio stuart russell demis hassabis sam altman express concern existential risk ai may 2023 geoffrey hinton announce resignation google order able freely speak risk ai without consider impact google notably mention risk ai takeover stress order avoid bad outcome establish safety guideline require cooperation among compete use ai 2023 many lead ai expert issue joint statement mitigate risk extinction ai global priority alongside societal scale risk pandemic nuclear war researcher however speak favor le dystopian view ai pioneer juergen schmidhuber sign joint statement emphasise 95 case ai research make human live long healthy easy tool use improve live also use bad actor also use bad actor andrew ng also argue mistake fall doomsday hype ai regulator benefit vest interest yann lecun scoff peer dystopian scenario supercharge misinformation even eventually human extinction early 2010 expert argue risk distant future warrant research human valuable perspective superintelligent machine however 2016 study current future risk possible solution become serious area research ethical machine alignment friendly ai machine design begin minimize risk make choice benefit human eliezer yudkowsky coin term argue develop friendly ai high research priority may require large investment must complete ai become existential risk machine intelligence potential use intelligence make ethical decision field machine ethic provide machine ethical principle procedure resolve ethical dilemma field machine ethic also call computational morality found aaai symposium 2005 approach include wendell wallach artificial moral agent stuart j russell three principle develop provably beneficial machine open source active organization ai open source community include hug face google eleutherai meta various ai model llama 2 mistral stable diffusion make open weight mean architecture train parameter weight publicly available open weight model freely fine tune allow company specialize data use case open weight model useful research innovation also misuse since fine tune build security measure object harmful request train away become ineffective researcher warn future ai model may develop dangerous capability potential drastically facilitate bioterrorism release internet cant delete everywhere need recommend pre release audit cost benefit analyse framework ai project ethical permissibility test design develop implement ai system ai framework care act framework contain sum value develop alan turing institute test project four main area respect dignity individual people connect people sincerely openly inclusively care wellbeing everyone protect social value justice public interest development ethical framework include decide upon asilomar conference montreal declaration responsible ai ieee ethic autonomous system initiative among others however principle go without criticism especially regard people choose contribute framework promotion wellbeing people community technology affect require consideration social ethical implication stag ai system design development implementation collaboration job role data scientist product manager data engineer domain expert delivery manager uk ai safety institute release 2024 test toolset call inspect ai safety evaluation available mit open source licence freely available github improve third party package use evaluate ai model range area include core knowledge ability reason autonomous capability regulation regulation ai development public sector policy law promote regulate ai therefore relate broad regulation algorithm regulatory policy landscape ai emerge issue jurisdiction globally accord ai index stanford annual number ai relate law pas 127 survey country jump one pas 2016 37 pas 2022 alone 2016 2020 30 country adopt dedicate strategy ai eu member state release national ai strategy canada china india japan mauritius russian federation saudi arabia unite arab emirate u vietnam others process elaborate ai strategy include bangladesh malaysia tunisia global partnership ai launch june 2020 state need ai develop accordance human right democratic value ensure public confidence trust technology henry kissinger eric schmidt daniel huttenlocher publish joint statement november 2021 call government commission regulate ai 2023 openai leader publish recommendation governance superintelligence believe may happen le 10 year 2023 unite nation also launch advisory body provide recommendation ai governance body comprise technology company executive government official academic 2022 ipsos survey attitude towards ai vary greatly country 78 chinese citizen 35 american agree product service use ai benefit drawback 2023 reutersipsos poll find 61 american agree 22 disagree ai pose risk humanity 2023 fox news poll 35 american think important additional 41 think somewhat important federal government regulate ai versus 13 respond important 8 respond important november 2023 first global ai safety summit hold bletchley park uk discus near far term risk ai possibility mandatory voluntary regulatory framework 28 country include unite state china european union issue declaration start summit call international co operation manage challenge risk ai may 2024 ai seoul summit 16 global ai tech company agree safety commitment development ai history study mechanical formal reason begin philosopher mathematician antiquity study logic lead directly alan turing theory computation suggest machine shuffle symbol simple 0 1 could simulate conceivable form mathematical reason along concurrent discovery cybernetics information theory neurobiology lead researcher consider possibility build electronic brain develop several area research would become part ai mccullouch pitt design artificial neuron 1943 turing influential 1950 paper compute machinery intelligence introduce turing test show machine intelligence plausible field ai research found workshop dartmouth college 1956 attendee become leader ai research 1960 student produce program press describe astonish computer learn checker strategy solve word problem algebra prove logical theorem speak english ai laboratory set number british u university latter 1950 early 1960 researcher 1960 1970 convince method would eventually succeed create machine general intelligence consider goal field 1965 herbert simon predict machine capable within twenty year work man 1967 marvin minsky agree write within generation problem create ai substantially solve however underestimate difficulty problem 1974 u british government cut exploratory research response criticism sir jam lighthill ongoing pressure u congress fund productive project minsky papert book perceptrons understand prove artificial neural network would never useful solve real world task thus discredit approach altogether ai winter period obtain fund ai project difficult follow early 1980 ai research revive commercial success expert system form ai program simulate knowledge analytical skill human expert 1985 market ai reach billion dollar time japan fifth generation computer project inspire u british government restore fund academic research however begin collapse lisp machine market 1987 ai fell disrepute second long last winter begin point ai fund go project use high level symbol represent mental object like plan goal belief know fact 1980 researcher begin doubt approach would able imitate process human cognition especially perception robotics learn pattern recognition begin look sub symbolic approach rodney brook reject representation general focus directly engineer machine move survive judea pearl lofti zadeh others develop method handle incomplete uncertain information make reasonable guess rather precise logic important development revival connectionism include neural network research geoffrey hinton others 1990 yann lecun successfully show convolutional neural network recognize handwritten digit first many successful application neural network ai gradually restore reputation late 1990 early 21t century exploit formal mathematical method find specific solution specific problem narrow formal focus allow researcher produce verifiable result collaborate field statistic economics mathematics 2000 solution develop ai researcher widely use although 1990 rarely describe ai however several academic researcher become concern ai long pursue original goal create versatile fully intelligent machine begin around 2002 found subfield artificial general intelligence agi several well fund institution 2010 deep learn begin dominate industry benchmark 2012 adopt throughout field many specific task method abandon deep learn success base hardware improvement fast computer graphic process unit cloud compute access large amount data include curated datasets imagenet deep learn success lead enormous increase interest fund ai amount machine learn research measure total publication increase 50 year 20152019 2016 issue fairness misuse technology catapult center stage machine learn conference publication vastly increase fund become available many researcher focus career issue alignment problem become serious field academic study late teen early 2020 agi company begin deliver program create enormous interest 2015 alphago develop deepmind beat world champion go player program teach rule game develop strategy gpt 3 large language model release 2020 openai capable generate high quality human like text program others inspire aggressive ai boom large company begin invest billion ai research accord ai impact 50 billion annually invest ai around 2022 u alone 20 new u computer science phd graduate specialize ai 800000 ai relate u job open exist 2022 philosophy define ai alan turing write 1950 propose consider question machine think advise change question whether machine think whether possible machinery show intelligent behaviour devise turing test measure ability machine simulate human conversation since observe behavior machine matter actually think literally mind turing note determine thing people usual polite convention everyone think russell norvig agree turing intelligence must define term external behavior internal structure however critical test require machine imitate human aeronautical engineer text write define goal field make machine fly exactly like pigeon fool pigeon ai founder john mccarthy agree write ai definition simulation human intelligence mccarthy define intelligence computational part ability achieve goal world another ai founder marvin minsky similarly describe ability solve hard problem lead ai textbook define study agent perceive environment take action maximize chance achieve define goal definition view intelligence term well define problem well define solution difficulty problem performance program direct measure intelligence machine philosophical discussion require may even possible another definition adopt google major practitioner field ai definition stipulate ability system synthesize information manifestation intelligence similar way define biological intelligence author suggest practice definition ai vague difficult define contention whether classical algorithm categorise ai many company early 2020 ai boom use term market buzzword often even actually use ai material way evaluate approach ai establish unify theory paradigm guide ai research history unprecedented success statistical machine learn 2010 eclipse approach much source especially business world use term ai mean machine learn neural network approach mostly sub symbolic soft narrow critic argue question may revisit future generation ai researcher symbolic ai limit symbolic ai gofai simulate high level conscious reason people use solve puzzle express legal reason mathematics highly successful intelligent task algebra iq test 1960 newell simon propose physical symbol system hypothesis physical symbol system necessary sufficient mean general intelligent action however symbolic approach fail many task human solve easily learn recognize object commonsense reason moravec paradox discovery high level intelligent task easy ai low level instinctive task extremely difficult philosopher hubert dreyfus argue since 1960 human expertise depend unconscious instinct rather conscious symbol manipulation feel situation rather explicit symbolic knowledge although argument ridicule ignore first present eventually ai research come agree issue resolve sub symbolic reason make many inscrutable mistake human intuition algorithmic bias critic noam chomsky argue continue research symbolic ai still necessary attain general intelligence part sub symbolic ai move away explainable ai difficult impossible understand modern statistical ai program make particular decision emerge field neuro symbolic ai attempt bridge two approach neat v scruffy neats hope intelligent behavior describe use simple elegant principle logic optimization neural network scruffies expect necessarily require solve large number unrelated problem neats defend program theoretical rigor scruffies rely mainly incremental test see work issue actively discus 1970 1980 eventually see irrelevant modern ai element soft v hard compute find provably correct optimal solution intractable many important problem soft compute set technique include genetic algorithm fuzzy logic neural network tolerant imprecision uncertainty partial truth approximation soft compute introduce late 1980 successful ai program 21t century example soft compute neural network narrow v general ai ai researcher divide whether pursue goal artificial general intelligence superintelligence directly solve many specific problem possible narrow ai hop solution lead indirectly field long term goal general intelligence difficult define difficult measure modern ai verifiable success focus specific problem specific solution experimental sub field artificial general intelligence study area exclusively machine consciousness sentience mind philosophy mind know whether machine mind consciousness mental state sense human be issue consider internal experience machine rather external behavior mainstream ai research consider issue irrelevant affect goal field build machine solve problem use intelligence russell norvig add additional project make machine conscious exactly way human one equip take however question become central philosophy mind also typically central question issue ai fiction consciousness david chalmers identify two problem understand mind name hard easy problem consciousness easy problem understand brain process signal make plan control behavior hard problem explain feel feel like anything assume right think truly feel like something dennett consciousness illusionism say illusion human information process easy explain human subjective experience difficult explain example easy imagine color blind person learn identify object field view red clear would require person know red look like computationalism functionalism computationalism position philosophy mind human mind information process system think form compute computationalism argue relationship mind body similar identical relationship software hardware thus may solution mindbody problem philosophical position inspire work ai researcher cognitive scientist 1960 originally propose philosopher jerry fodor hilary putnam philosopher john searle characterize position strong ai appropriately program computer right input output would thereby mind exactly sense human be mind searle counter assertion chinese room argument attempt show even machine perfectly simulate human behavior still reason suppose also mind ai welfare right difficult impossible reliably evaluate whether advance ai sentient ability feel degree significant chance give machine feel suffer may entitle certain right welfare protection measure similarly animal sapience set capacity relate high intelligence discernment self awareness may provide another moral basis ai right robot right also sometimes propose practical way integrate autonomous agent society 2017 european union consider grant electronic personhood capable ai system similarly legal status company would confer right also responsibility critic argue 2018 grant right ai system would downplay importance human right legislation focus user need rather speculative futuristic scenario also note robot lack autonomy take part society progress ai increase interest topic proponent ai welfare right often argue ai sentience emerge would particularly easy deny warn may moral blind spot analogous slavery factory farm could lead large scale suffer sentient ai create carelessly exploit future superintelligence singularity superintelligence hypothetical agent would posse intelligence far surpass bright gift human mind research artificial general intelligence produce sufficiently intelligent software might able reprogram improve improve software would even good improve lead j good call intelligence explosion vernor vinge call singularity however technology cannot improve exponentially indefinitely typically follow shape curve slow reach physical limit technology transhumanism robot designer han moravec cyberneticist kevin warwick inventor ray kurzweil predict human machine merge future cyborg capable powerful either idea call transhumanism root aldous huxley robert ettinger edward fredkin argue ai next stage evolution idea first propose samuel butler darwin among machine far back 1863 expand upon george dyson 1998 book darwin among machine evolution global intelligence fiction think capable artificial be appear storytelling device since antiquity persistent theme science fiction common trope work begin mary shelley frankenstein human creation become threat master include work arthur c clarke stanley kubrick 2001 space odyssey 1968 hal 9000 murderous computer charge discovery one spaceship well terminator 1984 matrix 1999 contrast rare loyal robot gort day earth stand still 1951 bishop alien 1986 le prominent popular culture isaac asimov introduce three law robotics many story notably multivac super intelligent computer asimov law often bring lay discussion machine ethic almost ai researcher familiar asimov law popular culture generally consider law useless many reason one ambiguity several work use ai force u confront fundamental question make u human show u artificial be ability feel thus suffer appear karel capek rur film ai ai ex machina well novel android dream electric sheep philip k dick dick consider idea understand human subjectivity alter technology create ai see also ai detection software software detect ai generate contentpages display short description redirect target behavior selection algorithm algorithm select action intelligent agent business process automation automation business process case base reason process solve new problem base solution similar past problem computational intelligence ability computer learn specific task data experimental observation digital immortality hypothetical concept store personality digital form emergent algorithm algorithm exhibit emergent behavior female gendering ai technology gender bias digital technologypages display short description redirect target glossary ai list definition term concept commonly use study ai intelligence amplification use information technology augment human intelligence mind upload hypothetical process digitally emulate brain robotic process automation form business process automation technology weak ai form ai wetware computer computer compose organic material explanatory note reference ai textbook two widely use textbook 2023 see open syllabus russell stuart j norvig peter 2021 ai modern approach 4 ed hoboken pearson isbn 978 0134610993 lccn 20190474 rich elaine knight kevin nair shivashankar b 2010 ai 3 ed new delhi tata mcgraw hill india isbn 978 0070087705 four widely use ai textbook 2008 history ai source read external link ai internet encyclopedia philosophy thomason richmond logic ai zalta edward n ed stanford encyclopedia philosophy ai bbc radio 4 discussion john agar alison adam igor aleksander time 8 december 2005\",\n          \"history ai ai begin antiquity myth story rumor artificial be endow intelligence consciousness master craftsman seed modern ai plant philosopher attempt describe process human think mechanical manipulation symbol work culminate invention programmable digital computer 1940 machine base abstract essence mathematical reason device idea behind inspire handful scientist begin seriously discus possibility build electronic brain field ai research found workshop hold campus dartmouth college u summer 1956 attend would become leader ai research decade many predict machine intelligent human would exist generation give million dollar make vision come true eventually become obvious researcher grossly underestimate difficulty project 1974 response criticism jam lighthill ongoing pressure u congress u british government stop fund undirected research ai seven year late visionary initiative japanese government inspire government industry provide ai billion dollar late 1980 investor become disillusion withdraw fund difficult year follow would late know ai winter ai criticize press avoid industry mid 2000 research fund continue grow name 1990 early 2000 machine learn apply many problem academia industry success due availability powerful computer hardware collection immense data set application solid mathematical method 2012 deep learn prove breakthrough technology eclipse method transformer architecture debut 2017 use produce impressive generative ai application investment ai boom 2020 precursor mythical fictional speculative precursor myth legend greek mythology talos giant construct bronze act guardian island crete would throw boulder ship invader would complete 3 circuit around island perimeter daily accord pseudo apollodorus bibliotheke hephaestus forge talos aid cyclops present automaton gift minos argonautica jason argonaut defeat way single plug near foot remove allow vital ichor flow body leave inanimate pygmalion legendary king sculptor greek mythology famously represent ovid metamorphose 10 book ovid narrative poem pygmalion become disgust woman witness way propoetides prostitute despite make offer temple venus ask goddess bring woman like statue carve medieval legend artificial be nature thing write swiss alchemist paracelsus describe procedure claim fabricate artificial man place sperm man horse dung feed arcanum man blood 40 day concoction become live infant early write account regard golem make find write eleazar ben judah worm early 13 century middle age believe animation golem could achieve insertion piece paper god name mouth clay figure unlike legendary automaton like brazen head golem unable speak takwin artificial creation life frequent topic ismaili alchemical manuscript especially attribute jabir ibn hayyan islamic alchemist attempt create broad range life work range plant animal faust second part tragedy johann wolfgang von goethe alchemically fabricate homunculus destine live forever flask make endeavor bear full human body upon initiation transformation however flask shatter homunculus die modern fiction 19 century idea artificial men think machine develop fiction mary shelley frankenstein karel capek rur rossum universal robot speculation samuel butler darwin among machine real world instance include edgar allan poe maelzel chess player ai common topic science fiction present automaton realistic humanoid automaton build craftsman many civilization include yan shi hero alexandria al jazari haroun al rashid jacques de vaucanson leonardo torres quevedo pierre jaquet droz wolfgang von kempelen old know automaton sacred statue ancient egypt greece faithful believe craftsman imbue figure real mind capable wisdom emotion hermes trismegistus write discover true nature god man able reproduce english scholar alexander neckham assert ancient roman poet virgil build palace automaton statue early modern period legendary automaton say posse magical ability answer question put late medieval alchemist proto protestant roger bacon purport fabricate brazen head develop legend wizard legend similar norse myth head mimir accord legend mimir know intellect wisdom behead aesir vanir war odin say embalm head herb speak incantation mimir head remain able speak wisdom odin odin keep head near counsel formal reason ai base assumption process human think mechanize study mechanical formal reason long history chinese indian greek philosopher develop structure method formal deduction first millennium bce idea develop century philosopher aristotle give formal analysis syllogism euclid whose element model formal reason al khwarizmi develop algebra give name algorithm european scholastic philosopher william ockham dun scotus spanish philosopher ramon llull 12321315 develop several logical machine devote production knowledge logical mean llull describe machine mechanical entity could combine basic undeniable truth simple logical operation produce machine mechanical mean way produce possible knowledge llull work great influence gottfried leibniz redevelop idea 17 century leibniz thomas hobbes rene descartes explore possibility rational think could make systematic algebra geometry hobbes famously write leviathan reason nothing reckon leibniz envision universal language reason characteristica universalis would reduce argumentation calculation would need disputation two philosopher two accountant would suffice take pencil hand slat say friend witness like let u calculate philosopher begin articulate physical symbol system hypothesis would become guide faith ai research study mathematical logic provide essential breakthrough make ai seem plausible foundation set work boole law think frege begriffsschrift build frege system russell whitehead present formal treatment foundation mathematics masterpiece principia mathematica 1913 inspire russell success david hilbert challenge mathematician 1920 30 answer fundamental question mathematical reason formalize question answer godel incompleteness proof turing machine church lambda calculus answer surprise two way first prove fact limit mathematical logic could accomplish second important ai work suggest within limit form mathematical reason could mechanize church turing thesis imply mechanical device shuffle symbol simple 0 1 could imitate conceivable process mathematical deduction key insight turing machine simple theoretical construct capture essence abstract symbol manipulation invention would inspire handful scientist begin discus possibility think machine computer science calculate machine design build antiquity throughout history many people include gottfried leibniz joseph marie jacquard charles babbage percy ludgate leonardo torres quevedo vannevar bush others ada lovelace speculate babbage machine think reason machine warn desirable guard possibility exaggerate idea arise power machine first modern computer massive machine second world war konrad zuse z3 alan turing heath robinson colossus atanasoff berry abc eniac university pennsylvania eniac base theoretical foundation lay alan turing develop john von neumann prove influential birth ai 1941 56 early research think machine inspire confluence idea become prevalent late 1930 1940 early 1950 recent research neurology show brain electrical network neuron fire nothing pulse norbert wiener cybernetics describe control stability electrical network claude shannon information theory describe digital signal ie nothing signal alan turing theory computation show form computation could describe digitally close relationship idea suggest might possible construct electronic brain 1940 50 handful scientist variety field mathematics psychology engineer economics political science explore several research direction would vital late ai research alan turing among first people seriously investigate theoretical possibility machine intelligence field ai research found academic discipline 1956 turing test 1950 turing publish landmark paper compute machinery intelligence speculate possibility create machine think paper note think difficult define devise famous turing test machine could carry conversation teleprinter indistinguishable conversation human reasonable say machine think simplify version problem allow turing argue convincingly think machine least plausible paper answer common objection proposition turing test first serious proposal philosophy ai artificial neural network walter pitt warren mcculloch analyze network idealize artificial neuron show might perform simple logical function 1943 first describe late researcher would call neural network paper influence turing paper computable number 1936 use similar two state boolean neuron first apply neuronal function one student inspire pitt mcculloch marvin minsky 24 year old graduate student time 1951 minsky dean edmonds build first neural net machine snarc minsky would late become one important leader innovator ai cybernetic robot experimental robot w grey walter turtle john hopkins beast build 1950 machine use computer digital electronics symbolic reason control entirely analog circuitry game ai 1951 use ferranti mark 1 machine university manchester christopher strachey write checker program dietrich prinz write one chess arthur samuel checker program subject 1959 paper study machine learn use game checker eventually achieve sufficient skill challenge respectable amateur game ai would continue use measure progress ai throughout history symbolic reason logic theorist access digital computer become possible mid fifty scientist instinctively recognize machine could manipulate number could also manipulate symbol manipulation symbol could well essence human think new approach create think machine 1955 allen newell future nobel laureate herbert simon create logic theorist help j c shaw program would eventually prove 38 first 52 theorem russell whitehead principia mathematica find new elegant proof simon say solve venerable mindbody problem explain system compose matter property mind early statement philosophical position john searle would late call strong ai machine contain mind human body symbolic reason paradigm introduce would dominate ai research fund middle 90 well inspire cognitive revolution dartmouth workshop dartmouth workshop 1956 pivotal event mark formal inception ai academic discipline organize marvin minsky john mccarthy support two senior scientist claude shannon nathan rochester ibm proposal conference state intend test assertion every aspect learn feature intelligence precisely describe machine make simulate term ai introduce john mccarthy workshop participant include ray solomonoff oliver selfridge trenchard arthur samuel allen newell herbert simon would create important program first decade ai research workshop newell simon debut logic theorist workshop moment ai gain name mission first major success key player widely consider birth ai cognitive revolution fall 1956 newell simon also present logic theorist meet special interest group information theory massachusetts institute technology mit meet noam chomsky discus generative grammar george miller describe landmark paper magical number seven plus minus two miller write leave symposium conviction intuitive rational experimental psychology theoretical linguistics computer simulation cognitive process piece large whole meet begin cognitive revolution interdisciplinary paradigm shift psychology philosophy computer science neuroscience inspire creation sub field symbolic ai generative linguistics cognitive science cognitive psychology cognitive neuroscience philosophical school computationalism functionalism field use relate tool model mind result discover one field relevant others cognitive approach allow researcher consider mental object like thought plan goal fact memory often analyze use high level symbol functional network object forbid unobservable early paradigm behaviorism symbolic mental object would become major focus ai research fund next several decade early success 1956 1974 program develop year dartmouth workshop people simply astonish computer solve algebra word problem prove theorem geometry learn speak english time would believe intelligent behavior machine possible researcher express intense optimism private print predict fully intelligent machine would build le 20 year government agency like defense advance research project agency darpa know arpa pour money field ai laboratory set number british u university latter 1950 early 1960 approach many successful program new direction late 50 1960 among influential reason search many early ai program use basic algorithm achieve goal like win game prove theorem proceed step step towards make move deduction search maze backtrack whenever reach dead end principal difficulty many problem number possible path maze astronomical situation know combinatorial explosion researcher would reduce search space use heuristic would eliminate path unlikely lead solution newell simon try capture general version algorithm program call general problem solver search program able accomplish impressive task like solve problem geometry algebra herbert gelernter geometry theorem prover 1958 symbolic automatic integrator saint write minsky student jam slagle 1961 program search goal subgoals plan action like strip system develop stanford control behavior robot shakey natural language important goal ai research allow computer communicate natural language like english early success daniel bobrow program student could solve high school algebra word problem semantic net represent concept eg house door nod relation among concept link nod eg first ai program use semantic net write ross quillian successful controversial version roger schank conceptual dependency theory joseph weizenbaum eliza could carry conversation realistic user occasionally fool think communicate human computer program see eliza effect fact eliza simply give can response repeat back say rephrase response grammar rule eliza first chatbot micro world late 60 marvin minsky seymour papert mit ai laboratory propose ai research focus artificially simple situation know micro world point successful science like physic basic principle often best understand use simplify model like frictionless plan perfectly rigid body much research focus block world consist color block various shape size array flat surface paradigm lead innovative work machine vision gerald sussman adolfo guzman david waltz invent constraint propagation especially patrick winston time minsky papert build robot arm could stack block bring block world life terry winograd shrdlu could communicate ordinary english sentence micro world plan operation execute perceptrons 1960 cognitive revolution highly influential fund direct towards laboratory research symbolic ai however one exception perceptron single layer neural network introduce 1958 frank rosenblatt schoolmate marvin minsky bronx high school science like ai researcher optimistic power predict perceptron may eventually able learn make decision translate language rosenblatt primarily fund office naval research bernard widrow student ted hoff build adaline 1960 madaline 1962 1000 adjustable weight group stanford research institute lead charles rosen alfred e ted brain build two neural network machine name minos 1960 ii 1963 mainly fund u army signal corp minos ii 6600 adjustable weight control sd 910 computer configuration name minos iii 1968 could classify symbol army map recognize hand print character fortran cod sheet neural network research early period involve build use bespeak hardware rather simulation digital computer hardware diversity particularly clear different technology use implement adjustable weight perceptron machine snarc use potentiometer move electric motor adaline use memistors adjust electroplate though also use simulation ibm 1620 computer minos machine use ferrite core multiple hole could individually block degree blockage represent weight however partly due lack result partly due competition symbolic ai research minos project run fund 1966 rosenblatt fail secure continue fund 1960 1969 research come sudden halt publication minsky papert 1969 book perceptrons suggest severe limitation perceptrons could rosenblatt prediction grossly exaggerate effect book virtually research fund connectionism 10 year competition government fund end victory symbolic ai approach neural network minsky work snarc become staunch objector pure connectionist ai widrow work adaline turn adaptive signal process sri group work minos turn symbolic ai robotics main problem inability train multilayered network version backpropagation already use field unknown researcher rosenblatt attempt gather fund build large perceptron machine die boat accident 1971 optimism first generation ai researcher make prediction work 1958 h simon allen newell within ten year digital computer world chess champion within ten year digital computer discover prove important new mathematical theorem 1965 h simon machine capable within twenty year work man 1967 marvin minsky within generation problem create ai substantially solve 1970 marvin minsky life magazine three eight year machine general intelligence average human finance june 1963 mit receive 2 2 million grant newly create advance research project agency arpa late know darpa money use fund project mac subsume ai group found minsky mccarthy five year early darpa continue provide 3 million year 70 darpa make similar grant newell simon program carnegie mellon university stanford university ai lab found john mccarthy 1963 another important ai laboratory establish edinburgh university donald michie 1965 four institution would continue main center ai research fund academia many year money give string attach j c r licklider director arpa believe organization fund people project allow researcher pursue whatever direction might interest create freewheel atmosphere mit give birth hacker culture hand approach last first ai winter 19741980 1970 ai subject critique financial setback ai researcher fail appreciate difficulty problem face tremendous optimism raise public expectation impossibly high promise result fail materialize fund target ai severely reduce lack success indicate technique use ai researcher time insufficient achieve goal setback affect growth progress field however fund cut impact handful major laboratory critique largely ignore general public interest field continue grow number researcher increase dramatically new idea explore logic program commonsense reason many area historian thomas haigh argue winter ai researcher nil nilsson describe period excite time work ai problem early seventy capability ai program limit even impressive could handle trivial version problem suppose solve program sense toy ai researcher begin run several limit would conquer decade late others still stymie field 2020 limit computer power enough memory process speed accomplish anything truly useful example ross quillian successful work natural language demonstrate vocabulary 20 word would fit memory han moravec argue 1976 computer still million time weak exhibit intelligence suggest analogy ai require computer power way aircraft require horsepower certain threshold impossible power increase eventually could become easy enough horsepower write anything fly intractability combinatorial explosion 1972 richard karp build stephen cook 1971 theorem show many problem solve exponential time find optimal solution problem require extraordinary amount computer time except problem trivial mean many toy solution use ai would never scale useful system commonsense knowledge reason many important ai application like vision natural language require enormous amount information world program need idea might look talk require program know thing world child researcher soon discover vast amount information one 1970 could build database large enough one know program might learn much information moravec paradox prove theorem solve geometry problem comparatively easy computer supposedly simple task like recognize face cross room without bump anything extremely difficult research vision robotics make little progress early 1970 frame qualification problem ai researcher like john mccarthy use logic discover could represent ordinary deduction involve plan default reason without make change structure logic develop new logic like non monotonic logic modal logic try solve problem decrease fund agency fund ai research british government darpa national research council nrc become frustrate lack progress eventually cut almost fund undirected ai research pattern begin 1966 automatic language process advisory committee alpac report criticize machine translation effort spend 20 million nrc end support 1973 lighthill report state ai research uk criticize failure ai achieve grandiose objective lead dismantle ai research country report specifically mention combinatorial explosion problem reason ai fail darpa deeply disappoint researcher work speech understand research program cmu cancel annual grant 3 million han moravec blame crisis unrealistic prediction colleague many researcher catch web increase exaggeration however another issue since passage mansfield amendment 1969 darpa increase pressure fund mission orient direct research rather basic undirected research fund creative freewheel exploration go 60 would come darpa instead direct money specific project clear objective autonomous tank battle management system major laboratory mit stanford cmu receive generous support u military withdraw place seriously impact budget cut thousand researcher outside institution many thousand join field unaffected philosophical ethical critique several philosopher strong objection claim make ai researcher one early john lucas argue godel incompleteness theorem show formal system computer program could never see truth certain statement human could hubert dreyfus ridicule break promise 1960 critique assumption ai argue human reason actually involve little symbol process great deal embody instinctive unconscious know john searle chinese room argument present 1980 attempt show program could say understand symbol use quality call intentionality symbol mean machine searle argue machine describe think critique take seriously ai researcher problem like intractability commonsense knowledge seem much immediate serious unclear difference know intentionality make actual computer program mit minsky say dreyfus searle misunderstand ignore dreyfus also teach mit give cold shoulder late say ai researcher dare see lunch joseph weizenbaum author eliza also outspoken critic dreyfus position deliberately make plain ai colleague treatment dreyfus way treat human unprofessional childish weizenbaum begin serious ethical doubt ai kenneth colby write computer program conduct psychotherapeutic dialogue base eliza weizenbaum disturb colby saw mindless program serious therapeutic tool feud begin situation help colby credit weizenbaum contribution program 1976 weizenbaum publish computer power human reason argue misuse ai potential devalue human life logic stanford cmu edinburgh logic introduce ai research early 1959 john mccarthy advice taker proposal 1963 j alan robinson discover simple method implement deduction computer resolution unification algorithm however straightforward implementation like attempt mccarthy student late 1960 especially intractable program require astronomical number step prove simple theorem fruitful approach logic develop 1970 robert kowalski university edinburgh soon lead collaboration french researcher alain colmerauer philippe roussel create successful logic program language prolog prolog use subset logic horn clause closely relate rule production rule permit tractable computation rule would continue influential provide foundation edward feigenbaum expert system continue work allen newell herbert simon would lead soar unify theory cognition critic logical approach note dreyfus human be rarely use logic solve problem experiment psychologist like peter wason eleanor rosch amos tversky daniel kahneman others provide proof mccarthy respond people irrelevant argue really need machine solve problem machine think people mit anti logic approach among critic mccarthy approach colleague across country mit marvin minsky seymour papert roger schank try solve problem like story understand object recognition require machine think like person order use ordinary concept like chair restaurant make illogical assumption people normally make unfortunately imprecise concept like hard represent logic gerald sussman observe use precise language describe essentially imprecise concept make precise schank describe anti logic approach scruffy oppose neat paradigm use mccarthy kowalski feigenbaum newell simon 1975 seminal paper minsky note many fellow researcher use kind tool framework capture common sense assumption something example use concept bird constellation fact immediately come mind might assume fly eat worm know fact always true deduction use fact logical structure set assumption part context everything say think call structure frame schank use version frame call script successfully answer question short story english emergence non monotonic logic logician rise challenge pat hay claim frame new syntax part first order logic note one two apparently minor detail give lot trouble however especially default meanwhile ray reiter admit conventional logic first order logic lack expressive power adequately represent knowledge require reason default propose augment first order logic close world assumption conclusion hold default contrary cannot show show assumption correspond common sense assumption make reason frame also show procedural equivalent negation failure prolog close world assumption formulate reiter first order notion meta notion however keith clark show negation finite failure understand reason implicitly definition first order logic include unique name assumption different term denote different individual late 1970 throughout 1980 variety logic extension first order logic develop negation failure logic program default reason generally collectively logic become know non monotonic logic boom 19801987 1980 form ai program call expert system adopt corporation around world knowledge become focus mainstream ai research government provide substantial fund japan fifth generation computer project u strategic compute initiative although symbolic knowledge representation logical reason produce useful application 80 still unable solve problem perception robotics learn common sense small number scientist engineer begin doubt symbolic approach would ever sufficient task develop approach connectionism robotics soft compute expert system become widely use expert system program answer question solve problem specific domain knowledge use logical rule derive knowledge expert early example develop edward feigenbaum student dendral begin 1965 identify compound spectrometer read mycin develop 1972 diagnose infectious blood disease demonstrate feasibility approach expert system restrict small domain specific knowledge thus avoid commonsense knowledge problem simple design make relatively easy program build modify place program prove useful something ai able achieve point 1980 expert system call xcon complete cmu digital equipment corporation enormous success save company 40 million dollar annually 1986 corporation around world begin develop deploy expert system 1985 spend billion dollar ai house ai department industry grow support include hardware company like symbolics lisp machine software company intellicorp aion government fund increase 1981 japanese ministry international trade industry set aside 850 million fifth generation computer project objective write program build machine could carry conversation translate language interpret picture reason like human be much chagrin scruffies choose prolog primary computer language project country respond new program uk begin 350 million alvey project consortium american company form microelectronics computer technology corporation mcc fund large scale project ai information technology darpa respond well found strategic compute initiative triple investment ai 1984 1988 knowledge revolution power expert system come expert knowledge contain part new direction ai research gain grind throughout 70 ai researcher begin suspect reluctantly violate scientific canon parsimony intelligence might well base ability use large amount diverse knowledge different way write pamela mccorduck great lesson 1970 intelligent behavior depend much deal knowledge sometimes quite detail knowledge domain give task lay knowledge base system knowledge engineer become major focus ai research 1980 1980 also saw birth cyc first attempt attack commonsense knowledge problem directly create massive database would contain mundane fact average person know douglas lenat start lead project argue shortcut way machine know mean human concept teach one concept time hand project expect complete many decade chess play program hitech deep think defeat chess master 1989 develop carnegie mellon university deep think development pave way deep blue revival neural network connectionism 1982 physicist john hopfield able prove form neural network call hopfield net could learn process information provably converge enough time fix condition breakthrough previously think nonlinear network would general evolve chaotically around time geoffrey hinton david rumelhart popularize method train neural network call backpropagation two discovery help revive exploration artificial neural network neural network along several similar model receive widespread attention 1986 publication parallel distribute process two volume collection paper edit rumelhart psychologist jam mcclelland new field christen connectionism considerable debate advocate symbolic ai connectionists 1990 yann lecun bell lab use convolutional neural network recognize handwritten digit system use widely 90 read zip cod personal check first genuinely useful application neural network robotics embody reason rodney brook han moravec others argue order show real intelligence machine need body need perceive move survive deal world sensorimotor skill essential high level skill commonsense reason cant efficiently implement use abstract symbolic reason ai solve problem perception mobility manipulation survival without use symbolic representation robotics researcher advocate build intelligence bottom precursor idea david marr come mit late 1970 successful background theoretical neuroscience lead group study vision reject symbolic approach mccarthy logic minsky frame argue ai need understand physical machinery vision bottom symbolic process take place marr work would cut short leukemia 1980 1990 paper elephant play chess robotics researcher brook take direct aim physical symbol system hypothesis argue symbol always necessary since world best model always exactly date always every detail know trick sense appropriately often enough 1980 1990 many cognitive scientist also reject symbol process model mind argue body essential reason theory call embody mind thesis bust second ai winter business community fascination ai rise fell 1980 classic pattern economic bubble dozen company fail perception business world technology viable damage ai reputation would last 21t century inside field little agreement reason ai failure fulfill dream human level intelligence capture imagination world 1960 together factor help fragment ai compete subfields focus particular problem approach sometimes even new name disguise tarnish pedigree ai next 20 year ai consistently deliver work solution specific isolate problem late 1990 use throughout technology industry although somewhat behind scene success due increase computer power collaboration field mathematical optimization statistic use high standard scientific accountability 2000 ai achieve old goal field cautious successful ever ai winter term ai winter coin researcher survive fund cut 1974 become concern enthusiasm expert system spiral control disappointment would certainly follow fear well found late 1980 early 1990 ai suffer series financial setback first indication change weather sudden collapse market specialize ai hardware 1987 desktop computer apple ibm steadily gain speed power 1987 become powerful expensive lisp machine make symbolics others long good reason buy entire industry worth half billion dollar demolish overnight eventually early successful expert system xcon prove expensive maintain difficult update could learn brittle ie could make grotesque mistake give unusual input fell prey problem qualification problem identify year early expert system prove useful special context late 1980 strategic compute initiative cut fund ai deeply brutally new leadership darpa decide ai next wave direct fund towards project seem likely produce immediate result 1991 impressive list goal pen 1981 japan fifth generation project meet indeed like carry casual conversation meet 2010 ai project expectation run much high actually possible 300 ai company shut go bankrupt acquire end 1993 effectively end first commercial wave ai 1994 hp newquist state brain maker immediate future ai commercial form seem rest part continue success neural network ai behind scene 1990 algorithm originally develop ai researcher begin appear part large system ai solve lot difficult problem solution prove useful throughout technology industry data mine industrial robotics logistics speech recognition bank software medical diagnosis google search engine field ai receive little credit success 1990 early 2000 many ai great innovation reduce status another item tool chest computer science nick bostrom explain lot cut edge ai filter general application often without call ai something become useful enough common enough label ai anymore many researcher ai 1990 deliberately call work name informatics knowledge base system cognitive system computational intelligence part may consider field fundamentally different ai also new name help procure fund commercial world least fail promise ai winter continue haunt ai research 2000 new york time report 2005 computer scientist software engineer avoid term ai fear view wild eye dreamer milestone moore law may 11 1997 deep blue become first computer chess play system beat reign world chess champion garry kasparov super computer specialize version framework produce ibm capable process twice many move per second first match deep blue lose reportedly 200000000 move per second 2005 stanford robot darpa grand challenge drive autonomously 131 mile along unrehearsed desert trail two year late team cmu darpa urban challenge autonomously navigate 55 mile urban environment respond traffic hazard adhere traffic law february 2011 jeopardy quiz show exhibition match ibm question answer system watson defeat two best jeopardy champion brad rutter ken jennings significant margin success due revolutionary new paradigm mostly tedious application engineer skill tremendous increase speed capacity computer 90 fact deep blue computer 10 million time fast ferranti mark 1 christopher strachey teach play chess 1951 dramatic increase measure moore law predict speed memory capacity computer double every two year result metaloxidesemiconductor mo transistor count double every two year fundamental problem raw computer power slowly overcome intelligent agent new paradigm call intelligent agent become widely accept 1990 although early researcher propose modular divide conquer approach ai intelligent agent reach modern form judea pearl allen newell leslie p kaelbling others bring concept decision theory economics study ai economist definition rational agent marry computer science definition object module intelligent agent paradigm complete intelligent agent system perceive environment take action maximize chance success definition simple program solve specific problem intelligent agent human be organization human be firm intelligent agent paradigm define ai research study intelligent agent generalization early definition ai go beyond study human intelligence study kind intelligence paradigm give researcher license study isolate problem find solution verifiable useful provide common language describe problem share solution field also use concept abstract agent like economics control theory hop complete agent architecture like newell soar would one day allow researcher build versatile intelligent system interact intelligent agent probabilistic reason great rigor ai researcher begin develop use sophisticate mathematical tool ever past widespread realization many problem ai need solve already work researcher field like mathematics electrical engineer economics operation research share mathematical language allow high level collaboration establish successful field achievement result measurable provable ai become rigorous scientific discipline judea pearl influential 1988 book bring probability decision theory ai among many new tool use bayesian network hide markov model information theory stochastic model classical optimization precise mathematical description also develop computational intelligence paradigm like neural network evolutionary algorithm deep learn big data 20112020 first decade 21t century access large amount data know big data cheap fast computer advance machine learn technique successfully apply many problem throughout economy fact mckinsey global institute estimate famous paper big data next frontier innovation competition productivity 2009 nearly sector u economy least average 200 terabyte store data 2016 market ai relate product hardware software reach 8 billion new york time report interest ai reach frenzy application big data begin reach field well train model ecology various application economics advance deep learn particularly deep convolutional neural network recurrent neural network drive progress research image video process text analysis even speech recognition first global ai safety summit hold bletchley park uk november 2023 discus short long term risk ai possibility mandatory voluntary regulatory framework twenty eight country include unite state china european union issue declaration start summit call international co operation manage challenge risk ai deep learn deep learn branch machine learn model high level abstraction data use deep graph many process layer accord universal approximation theorem deep ness necessary neural network able approximate arbitrary continuous function even many problem common shallow network overfitting deep network help avoid deep neural network able realistically generate much complex model compare shallow counterpart however deep learn problem common problem recurrent neural network vanish gradient problem gradient pas layer gradually shrink literally disappear round zero many method develop approach problem long short term memory unit state art deep neural network architecture sometimes even rival human accuracy field like computer vision specifically thing like modify national institute standard technology mnist database traffic sign recognition language process engine power smart search engine easily beat human answer general trivia question ibm watson recent development deep learn produce surprise result compete human specifically game like go doom first person shooter game spark controversy big data big data refer collection data cannot capture manage process conventional software tool within certain time frame massive amount decision make insight process optimization capability require new process model book big data era victor meyer schonberg kenneth cooke big data mean instead random analysis sample survey data use analysis 5v characteristic big data propose ibm volume velocity variety value veracity strategic significance big data technology master huge data information specialize meaningful data word big data liken industry key realize profitability industry increase process capability data realize value add data process large language model ai boom 2020present ai boom start initial development key architecture algorithm transformer architecture 2017 lead scale development large language model exhibit human like trait reason cognition attention creativity new ai era say begin around 20222023 public release scale large language model llm chatgpt large language model 2017 transformer architecture propose google researcher exploit attention mechanism late become widely use large language model foundation model large language model train vast quantity unlabeled data adapt wide range downstream task begin develop 2018 model gpt 3 release openai 2020 gato release deepmind 2022 describe important achievement machine learn 2023 microsoft research test gpt 4 large language model large variety task conclude could reasonably view early yet still incomplete version artificial general intelligence agi system see also history artificial neural network history knowledge representation reason history natural language process outline ai progress ai timeline ai timeline machine learn note reference\",\n          \"data science institute research institute imperial college london found may 2014 institute one five global institute imperial college london alongside institute global health innovation energy future lab institute security science technology grantham institute climate change environment data science institute partnership international industry academia formal investment chinese multinational telecom company huawei multinational consultancy kpmg zhejiang university china goal institute enhance multidisciplinary data science research across whole imperial college coordinate promote data drive research education activity activity cover area across college include engineer medicine natural science business institute house custom build large scale immersive data visualization facility call kpmg data observatory resolution 132 megapixels think large system europe reference external link official website httpswwwimperialacukresearch innovationabout imperial researchglobal institute\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tokens_list\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "articles.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYl9n2sAGJDi"
      },
      "source": [
        "#Text Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbVf6eghGIUC"
      },
      "outputs": [],
      "source": [
        "# AI_text = \"\"\n",
        "# DS_text = \"\"\n",
        "# DB_text = \"\"\n",
        "\n",
        "# for t in df[:20][\"Text\"]:\n",
        "#   AI_text += t\n",
        "# for t in df[20:40][\"Text\"]:\n",
        "#   DS_text += t\n",
        "# for t in df[40:][\"Text\"]:\n",
        "#   DB_text += t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_jOzbI8GPnx"
      },
      "outputs": [],
      "source": [
        "# print(\"---------------- Artificial Intelligence ----------------\")\n",
        "# total = analyze_text_full(AI_text)\n",
        "# print(\" \")\n",
        "\n",
        "# print(\"---------------- Data Science ----------------\")\n",
        "# total = analyze_text_full(DS_text)\n",
        "# print(\" \")\n",
        "\n",
        "# print(\"---------------- Database ----------------\")\n",
        "# total = analyze_text_full(DB_text)\n",
        "# print(\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ-AQmzUHsRA"
      },
      "source": [
        "#Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izRrIgm8Hufz"
      },
      "outputs": [],
      "source": [
        "# Figure out characters\n",
        "char_unique = set()\n",
        "\n",
        "for id, article in articles.iterrows():\n",
        "  text = article[\"Tokens\"]\n",
        "\n",
        "  for char in text:\n",
        "    char_unique.add(char)\n",
        "\n",
        "# sorted(char_unique)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HicEL2C-sQNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "881340e8-226e-4874-ccc2-9578886a7c57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Term    Frequency\n",
              "0       95           {0, 2, 4, 5, 9, 11, 12, 15, 17, 18, 21, 22, 23...\n",
              "00      1                                                         {12}\n",
              "00023   1                                                         {30}\n",
              "0004    1                                                         {12}\n",
              "001     1                                                         {25}\n",
              "                                           ...                        \n",
              "zoom    1                                                         {35}\n",
              "zoon    1                                                         {32}\n",
              "zurich  1                                                         {18}\n",
              "zuse    1                                                          {5}\n",
              "zwei    1                                                         {19}\n",
              "Name: Dictionary, Length: 11684, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Dictionary</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Term</th>\n",
              "      <th>Frequency</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>95</th>\n",
              "      <td>{0, 2, 4, 5, 9, 11, 12, 15, 17, 18, 21, 22, 23...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00</th>\n",
              "      <th>1</th>\n",
              "      <td>{12}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>00023</th>\n",
              "      <th>1</th>\n",
              "      <td>{30}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0004</th>\n",
              "      <th>1</th>\n",
              "      <td>{12}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>001</th>\n",
              "      <th>1</th>\n",
              "      <td>{25}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoom</th>\n",
              "      <th>1</th>\n",
              "      <td>{35}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zoon</th>\n",
              "      <th>1</th>\n",
              "      <td>{32}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zurich</th>\n",
              "      <th>1</th>\n",
              "      <td>{18}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zuse</th>\n",
              "      <th>1</th>\n",
              "      <td>{5}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zwei</th>\n",
              "      <th>1</th>\n",
              "      <td>{19}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11684 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "word_freq = {}\n",
        "word_appearances = {}\n",
        "\n",
        "for id, article in articles.iterrows():\n",
        "  text = article[\"Tokens\"]\n",
        "\n",
        "  for word in word_tokenize(text):\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    if word not in word_appearances:\n",
        "      word_appearances[word] = set()\n",
        "    word_appearances[word].add(id)\n",
        "\n",
        "word_freq = [(word, freq) for word, freq in word_freq.items()]\n",
        "multi_index = pd.MultiIndex.from_tuples(word_freq, names=['Term', 'Frequency'])\n",
        "\n",
        "wiki_index = pd.Series(list(word_appearances.values()), index=multi_index, name='Dictionary')\n",
        "wiki_index = wiki_index.sort_index()\n",
        "\n",
        "vocab = wiki_index.index.get_level_values(0).tolist()\n",
        "\n",
        "wiki_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyF8dgw-IQ-M"
      },
      "outputs": [],
      "source": [
        "# # handle alphanumerics?\n",
        "# for word in wiki_index.index:\n",
        "#   if bool(re.match(r'\\d+', word)):\n",
        "#     print(word, wiki_index.loc[word]['Postings'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImXzwiYrFXzF"
      },
      "source": [
        "#TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir9_rz92FAC3"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(vocabulary=vocab)\n",
        "tfidf_matrix = tfidf.fit_transform(articles[\"Tokens\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_IRS(query, tfidf):\n",
        "  query_expanded = expand_query_with_synonyms(query)\n",
        "  query_clean = preprocess(clean_text(query_expanded)).split()\n",
        "\n",
        "  # combine tfidf of query to make size (1, vocab)\n",
        "  matrix = tfidf.transform(query_clean)\n",
        "  combined = np.array(matrix.sum(axis=0))\n",
        "  query_matrix = csr_matrix(combined)\n",
        "\n",
        "  scores_tfidf = {i: 0 for i in range(tfidf_matrix.shape[0])}\n",
        "\n",
        "  scores = [cosine_similarity(vector, query_matrix)[0][0] for vector in tfidf_matrix]\n",
        "\n",
        "  scores_tfidf = pd.DataFrame({'score': scores})\n",
        "  sorted_tfidf = scores_tfidf.sort_values(by='score', ascending = False)\n",
        "\n",
        "  return get_gains(sorted_tfidf)"
      ],
      "metadata": {
        "id": "MIGrb5wbEO4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdSuDwNTWZII"
      },
      "source": [
        "#Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONXRP2DCd-bd"
      },
      "outputs": [],
      "source": [
        "emb_size = 3\n",
        "\n",
        "flattened = [sentence for article in articles[\"Tokens_list\"] for sentence in article]\n",
        "w2v_cbow = Word2Vec(sentences=flattened, vector_size=emb_size, window=5, min_count=1, sg=0)\n",
        "w2v_skip = Word2Vec(sentences=flattened, vector_size=emb_size, window=5, min_count=1, sg=1)\n",
        "\n",
        "articles[\"Embeddings_cbow\"] = articles[\"Tokens_list\"].apply(lambda article: pad_sents(article, longest_sent))\n",
        "articles[\"Embeddings_cbow\"] = articles[\"Embeddings_cbow\"].apply(lambda article: np.array([[w2v_cbow.wv[word] if word in w2v_cbow.wv else np.zeros(emb_size) for word in sent] for sent in article]))\n",
        "\n",
        "articles[\"Embeddings_skip\"] = articles[\"Tokens_list\"].apply(lambda article: pad_sents(article, longest_sent))\n",
        "articles[\"Embeddings_skip\"] = articles[\"Embeddings_skip\"].apply(lambda article: np.array([[w2v_skip.wv[word] if word in w2v_skip.wv else np.zeros(emb_size) for word in sent] for sent in article]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def w2v_IRS(embeddings, query, w2v, longest, emb):\n",
        "  query_expanded = expand_query_with_synonyms(query)\n",
        "  query_clean = preprocess(clean_text(query_expanded)).split()\n",
        "\n",
        "  # pad query\n",
        "  query_padded = query_clean.copy()\n",
        "  while len(query_padded) < longest_sent:\n",
        "      query_padded.append(\"<oov>\")\n",
        "\n",
        "  # replace with embedding\n",
        "  query_vector = np.array([w2v.wv[word] if word in w2v.wv else np.zeros(emb) for word in query_padded])\n",
        "\n",
        "  scores_w2v = {i: 0 for i in range(embeddings.shape[0])}\n",
        "\n",
        "  for id in range(embeddings.shape[0]):\n",
        "    scores = []\n",
        "    for vector in embeddings[id]:\n",
        "      similarity = cosine_similarity(vector, query_vector)[0][0]\n",
        "      scores.append(similarity)\n",
        "    average_similarity_score = sum(scores) / len(scores)\n",
        "    scores_w2v[id] = average_similarity_score\n",
        "\n",
        "  scores_w2v = pd.DataFrame({'score': scores_w2v})\n",
        "  sorted_w2v = scores_w2v.sort_values(by='score', ascending = False)\n",
        "\n",
        "  return get_gains(sorted_w2v)"
      ],
      "metadata": {
        "id": "hC5PJd4fOIgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDI2h3b9FVQC"
      },
      "source": [
        "#Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhnbYhnvFcoa"
      },
      "source": [
        "#Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scores_tfidf = {i: 0 for i in range(60)}\n",
        "\n",
        "# scores = [cosine_similarity(vector, query_matrix)[0][0] for vector in tfidf_matrix]\n",
        "\n",
        "# scores_tfidf = pd.DataFrame({'score': scores})\n",
        "# sorted_tfidf = scores_tfidf.sort_values(by='score', ascending = False)\n",
        "\n",
        "# sorted_tfidf = get_gains(sorted_tfidf)\n",
        "# print(sorted_tfidf[:10])"
      ],
      "metadata": {
        "id": "4UlNQ-9pOQN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores_w2v = {i: 0 for i in range(60)}\n",
        "# # import sys\n",
        "\n",
        "# # np.concatenate(np.vstack(df[\"Embeddings\"][42][1]))\n",
        "\n",
        "# # Set printing options\n",
        "# # np.set_printoptions(precision=4, suppress=True, threshold=sys.maxsize)\n",
        "\n",
        "# for id in range(articles.shape[0]):\n",
        "# # for id in range(2,3):\n",
        "#   scores = []\n",
        "#   for vector in articles[\"Embeddings\"][id]:\n",
        "#     # vector = np.concatenate(np.vstack(sent))\n",
        "\n",
        "#     padded_query_vector_tok = query_vector_tok.copy()\n",
        "#     padded_vector = vector.copy()\n",
        "\n",
        "#     if len(vector) > len(query_vector_tok):\n",
        "#         padded_query_vector_tok = np.pad(query_vector_tok, (0, len(vector) - len(query_vector_tok)), mode='constant')\n",
        "#     elif len(query_vector_tok) > len(vector):\n",
        "#         padded_vector = np.pad(vector, (0, len(query_vector_tok) - len(vector)), mode='constant')\n",
        "#     print(len(padded_vector)/100, len(padded_query_vector_tok)/100)\n",
        "#     if (np.isnan(padded_query_vector_tok).any()):\n",
        "#       print(True)\n",
        "#     if(np.isnan(padded_vector).any()):\n",
        "#       print(True)\n",
        "#     norm_vector = np.linalg.norm(vector)\n",
        "\n",
        "#     Handle zero vectors\n",
        "#     if norm_query == 0 or norm_vector == 0:\n",
        "#         scores.append(0)\n",
        "#         continue\n",
        "#     unit_vector = vector / norm_vector\n",
        "#     if np.isnan(padded_vector).any():\n",
        "#       print(True)\n",
        "#     print(padded_vector)\n",
        "\n",
        "#     similarity = cosine_similarity(vector, query_vector_tok)[0][0]\n",
        "#     scores.append(similarity)\n",
        "#   #   # print(len(vector)/100)\n",
        "#   #   # print(vector[[-1]])\n",
        "#   #   # print(similarity)\n",
        "#   average_similarity_score = sum(scores) / len(scores)\n",
        "#   scores_w2v[id] = average_similarity_score\n",
        "\n",
        "#   # # print(len(vector))\n",
        "\n",
        "# scores_w2v = pd.DataFrame({'score': scores_w2v})\n",
        "# sorted_w2v = scores_w2v.sort_values(by='score', ascending = False)\n",
        "# # sorted\n",
        "# # sorted = scores_df.sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "BRPAy7gsCzqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fQFp3ZPTK4f"
      },
      "outputs": [],
      "source": [
        "# scores_w2v = {i: 0 for i in range(60)}\n",
        "\n",
        "# for id in range(articles.shape[0]):\n",
        "#   scores = []\n",
        "#   for vector in articles[\"Embeddings\"][id]:\n",
        "#     similarity = cosine_similarity(vector, query_vector)[0][0]\n",
        "#     scores.append(similarity)\n",
        "#   average_similarity_score = sum(scores) / len(scores)\n",
        "#   scores_w2v[id] = average_similarity_score\n",
        "\n",
        "# scores_w2v = pd.DataFrame({'score': scores_w2v})\n",
        "# sorted_w2v = scores_w2v.sort_values(by='score', ascending = False)\n",
        "\n",
        "# sorted_w2v = get_gains(sorted_w2v)\n",
        "# print(sorted_w2v[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "Ax8ztbwy5Vsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_word2vec = []\n",
        "eval_tfidf = []"
      ],
      "metadata": {
        "id": "mthVgzGPsJ8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7OCiFrDuNDn",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a28e29-9ec9-49a2-c95d-f851126856b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG score (WORD2VEC):  0.2748454742404674\n",
            "\n",
            "\n",
            "NDCG score (TFIDF):  0.7761080040656038\n"
          ]
        }
      ],
      "source": [
        "my_query = \"what is artificial intelligence?\"\n",
        "\n",
        "data = {'docID': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
        "      'ideal_gain': [3, 2, 2, 3, 0, 2, 3, 1, 2, 2, 1, 0, 0, 2, 2, 2, 1, 1, 2, 0]}\n",
        "\n",
        "df1 = pd.DataFrame(data)\n",
        "\n",
        "df_query1 = finalize_df(df1)\n",
        "df_word2vec_query1 = w2v_IRS(articles[\"Embeddings_cbow\"], my_query, w2v_cbow, longest_sent, emb_size)\n",
        "df_word2vec_query1 = df_word2vec_query1.sort_index()\n",
        "\n",
        "combined = combine_df(df_query1, df_word2vec_query1)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_word2vec.append(eval)\n",
        "\n",
        "print(\"NDCG score (WORD2VEC): \", eval)\n",
        "print(\"\\n\")\n",
        "\n",
        "df_query1 = finalize_df(df1)\n",
        "df_tfidf_query1 = tfidf_IRS(my_query, tfidf)\n",
        "df_tfidf_query1 = df_tfidf_query1.sort_index()\n",
        "\n",
        "combined = combine_df(df_query1, df_tfidf_query1)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_tfidf.append(eval)\n",
        "\n",
        "print(\"NDCG score (TFIDF): \", eval)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_query = \"what is data science?\"\n",
        "\n",
        "data = {'docID': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
        "      'ideal_gain': [3, 1, 2, 0, 1, 2, 2, 2, 2, 0, 1, 2, 2, 0, 1, 1, 2, 1, 2, 2]}\n",
        "\n",
        "df2 = pd.DataFrame(data)\n",
        "\n",
        "df_query2 = finalize_df(df2)\n",
        "df_word2vec_query2 = w2v_IRS(articles[\"Embeddings_cbow\"], my_query, w2v_cbow, longest_sent, emb_size)\n",
        "df_word2vec_query2 = df_word2vec_query2.sort_index()\n",
        "\n",
        "combined = combine_df(df_query2, df_word2vec_query2)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_word2vec.append(eval)\n",
        "\n",
        "print(\"NDCG score (WORD2VEC): \", eval)\n",
        "print(\"\\n\")\n",
        "\n",
        "df_query2 = finalize_df(df2)\n",
        "df_tfidf_query2 = tfidf_IRS(my_query, tfidf)\n",
        "df_tfidf_query2 = df_tfidf_query2.sort_index()\n",
        "\n",
        "combined = combine_df(df_query2, df_tfidf_query2)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_tfidf.append(eval)\n",
        "\n",
        "print(\"NDCG score (TFIDF): \", eval)"
      ],
      "metadata": {
        "id": "pQr-xWkmC2eS",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2786b5-bec1-4c73-ea9c-f9f3c0cff8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG score (WORD2VEC):  0.3028125992806813\n",
            "\n",
            "\n",
            "NDCG score (TFIDF):  0.7523672216429602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_query = \"what is a database?\"\n",
        "\n",
        "data = {'docID': [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
        "        'ideal_gain': [3, 0, 3, 1, 2, 3, 3, 3, 2, 2, 1, 3, 2, 1, 2, 1, 3, 1, 3, 2]}\n",
        "\n",
        "df3 = pd.DataFrame(data)\n",
        "\n",
        "df_query3 = finalize_df(df3)\n",
        "df_word2vec_query3 = w2v_IRS(articles[\"Embeddings_cbow\"], my_query, w2v_cbow, longest_sent, emb_size)\n",
        "df_word2vec_query3 = df_word2vec_query3.sort_index()\n",
        "\n",
        "combined = combine_df(df_query3, df_word2vec_query3)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_word2vec.append(eval)\n",
        "\n",
        "print(\"NDCG score (WORD2VEC): \", eval)\n",
        "print(\"\\n\")\n",
        "\n",
        "df_query3 = finalize_df(df3)\n",
        "df_tfidf_query3 = tfidf_IRS(my_query, tfidf)\n",
        "df_tfidf_query3 = df_tfidf_query3.sort_index()\n",
        "\n",
        "combined = combine_df(df_query3, df_tfidf_query3)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_tfidf.append(eval)\n",
        "\n",
        "print(\"NDCG score (TFIDF): \", eval)"
      ],
      "metadata": {
        "id": "OSPe8wg7DXYk",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8379bf79-5cc2-442d-ddf8-9ca46f681a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG score (WORD2VEC):  0.4159752893739773\n",
            "\n",
            "\n",
            "NDCG score (TFIDF):  0.732952443075378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_query = \"what is machine learning?\"\n",
        "\n",
        "docID = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
        "Rank = [2, 0, 0, 3, 0, 0, 1, 0, 2, 3, 0, 0, 1, 2, 1, 3, 1, 0, 3, 1]\n",
        "\n",
        "df4 = pd.DataFrame({'docID': docID, 'ideal_gain': Rank})\n",
        "\n",
        "df_query4 = finalize_df(df4)\n",
        "df_word2vec_query4 = w2v_IRS(articles[\"Embeddings_cbow\"], my_query, w2v_cbow, longest_sent, emb_size)\n",
        "df_word2vec_query4 = df_word2vec_query4.sort_index()\n",
        "\n",
        "combined = combine_df(df_query4, df_word2vec_query4)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_word2vec.append(eval)\n",
        "\n",
        "print(\"NDCG score (WORD2VEC): \", eval)\n",
        "print(\"\\n\")\n",
        "\n",
        "df_query4 = finalize_df(df4)\n",
        "df_tfidf_query4 = tfidf_IRS(my_query, tfidf)\n",
        "df_tfidf_query4 = df_tfidf_query4.sort_index()\n",
        "\n",
        "combined = combine_df(df_query4, df_tfidf_query4)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_tfidf.append(eval)\n",
        "\n",
        "print(\"NDCG score (TFIDF): \", eval)"
      ],
      "metadata": {
        "id": "fozoP5FeDlWr",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf89910d-a73f-40ac-9c5c-50d9e2cf6841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG score (WORD2VEC):  0.2049956614487451\n",
            "\n",
            "\n",
            "NDCG score (TFIDF):  0.6369318083805976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_query = \"data storage?\"\n",
        "\n",
        "data = {'docID': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,  21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
        "        'ideal_gain': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 2, 0, 0, 3, 2, 1, 3, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 3, 0, 3, 1, 0, 3, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1]}\n",
        "\n",
        "df_query5 = pd.DataFrame(data)\n",
        "\n",
        "df_word2vec_query5 = w2v_IRS(articles[\"Embeddings_cbow\"], my_query, w2v_cbow, longest_sent, emb_size)\n",
        "df_word2vec_query5 = df_word2vec_query5.sort_index()\n",
        "\n",
        "combined = combine_df(df_query5, df_word2vec_query5)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_word2vec.append(eval)\n",
        "\n",
        "print(\"NDCG score (WORD2VEC): \", eval)\n",
        "print(\"\\n\")\n",
        "\n",
        "df_query5 = pd.DataFrame(data)\n",
        "df_tfidf_query5 = tfidf_IRS(my_query, tfidf)\n",
        "df_tfidf_query5 = df_tfidf_query5.sort_index()\n",
        "\n",
        "combined = combine_df(df_query5, df_tfidf_query5)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_tfidf.append(eval)\n",
        "\n",
        "print(\"NDCG score (TFIDF): \", eval)"
      ],
      "metadata": {
        "id": "CvbVn8V_Dxhw",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933ff769-6096-47d8-9f57-4243d9c12dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG score (WORD2VEC):  0.33927626990687654\n",
            "\n",
            "\n",
            "NDCG score (TFIDF):  0.4851628934216609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_query = \"database management\"\n",
        "\n",
        "data = {'docID': [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
        "        'ideal_gain': [1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 3]}\n",
        "\n",
        "df6 = pd.DataFrame(data)\n",
        "\n",
        "df_query6 = finalize_df(df6)\n",
        "df_word2vec_query6 = w2v_IRS(articles[\"Embeddings_cbow\"], my_query, w2v_cbow, longest_sent, emb_size)\n",
        "df_word2vec_query6 = df_word2vec_query6.sort_index()\n",
        "\n",
        "combined = combine_df(df_query6, df_word2vec_query6)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_word2vec.append(eval)\n",
        "\n",
        "print(\"NDCG score (WORD2VEC): \", eval)\n",
        "print(\"\\n\")\n",
        "\n",
        "df_query6 = finalize_df(df6)\n",
        "df_tfidf_query6 = tfidf_IRS(my_query, tfidf)\n",
        "df_tfidf_query6 = df_tfidf_query6.sort_index()\n",
        "\n",
        "combined = combine_df(df_query6, df_tfidf_query6)\n",
        "combined = combined.sort_values(by = 'ideal_gain', ascending = False)\n",
        "\n",
        "ideal_gain = combined['ideal_gain'].tolist()\n",
        "gain = combined['gain'].tolist()\n",
        "\n",
        "y_true = [ideal_gain]\n",
        "y_score = [gain]\n",
        "\n",
        "k_val = ideal_gain.index(0)\n",
        "\n",
        "eval = ndcg_score(y_true, y_score, k = k_val)\n",
        "eval_tfidf.append(eval)\n",
        "\n",
        "print(\"NDCG score (TFIDF): \", eval)"
      ],
      "metadata": {
        "id": "9lavG_VzD7iO",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a071053-1324-4504-b7d3-b52451f8317f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG score (WORD2VEC):  0.15817712631939626\n",
            "\n",
            "\n",
            "NDCG score (TFIDF):  0.28496255468093407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Values"
      ],
      "metadata": {
        "id": "0n8NAjdLCACm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_word2vec)\n",
        "word2vec_e = sum(eval_word2vec) / len(eval_word2vec)\n",
        "print('\\n')\n",
        "print(word2vec_e)"
      ],
      "metadata": {
        "id": "kvy35p5tuDEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14b15e5-b447-4cde-a74f-41f180b88eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2748454742404674, 0.3028125992806813, 0.4159752893739773, 0.2049956614487451, 0.33927626990687654, 0.15817712631939626]\n",
            "\n",
            "\n",
            "0.2826804034283573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_tfidf)\n",
        "tfidf_e = sum(eval_tfidf) / len(eval_tfidf)\n",
        "print('\\n')\n",
        "print(tfidf_e)"
      ],
      "metadata": {
        "id": "IjqCuoiguTyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e2f143-322a-420a-ec51-2dbf38317029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7761080040656038, 0.7523672216429602, 0.732952443075378, 0.6369318083805976, 0.4851628934216609, 0.28496255468093407]\n",
            "\n",
            "\n",
            "0.6114141542111892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interface"
      ],
      "metadata": {
        "id": "_LS01_zwALMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = widgets.Text(description=\"Enter query:\", value='')\n",
        "\n",
        "button = widgets.Button(description='Generate Results')\n",
        "\n",
        "amount = 10\n",
        "\n",
        "def on_button_click(b, articles_df, amount):\n",
        "  query = text_input.value\n",
        "\n",
        "  result_tfidf = tfidf_IRS(query, tfidf)\n",
        "  # result_w2v = w2v_IRS(articles[\"Embeddings_cbow\"], query, w2v_cbow, longest_sent, emb_size)\n",
        "  # result = w2v_IRS(articles[\"Embeddings_skip\"], query, w2v_skip, longest_sent, emb_size)\n",
        "\n",
        "  clear_output()\n",
        "  display(text_input, button)\n",
        "  for i, id in zip(range(1, amount+1), result_tfidf[:amount].index):\n",
        "    print(str(i) + '. ' + articles_df['Title'][id] + ' \\nURL: ' + articles_df['URL'][id])\n",
        "\n",
        "button.on_click(lambda b: on_button_click(b, articles, amount))\n",
        "\n",
        "display(text_input, button)"
      ],
      "metadata": {
        "id": "wlMtND1JANOY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428,
          "referenced_widgets": [
            "c23a25608b6648b1bda791854b516024",
            "0d986531f27241e096ff29a639441369",
            "d961eadfb879470999ce04bf5de69a38",
            "6f80f1ed1b9e401fb7867f0ea3d35918",
            "a02763ef811d4f6bb864d28eae42e26b",
            "a57b5c95054140c6bdec6581a8993809"
          ]
        },
        "outputId": "5c69a180-350d-4024-c778-f706a357f574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='AI career options', description='Enter query:')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c23a25608b6648b1bda791854b516024"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Generate Results', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f80f1ed1b9e401fb7867f0ea3d35918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Artificial intelligence \n",
            "URL: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "2. Regulation of artificial intelligence \n",
            "URL: https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence\n",
            "3. Ethics of artificial intelligence \n",
            "URL: https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence\n",
            "4. Applications of artificial intelligence \n",
            "URL: https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence\n",
            "5. Artificial general intelligence \n",
            "URL: https://en.wikipedia.org/wiki/Artificial_general_intelligence\n",
            "6. Generative artificial intelligence \n",
            "URL: https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n",
            "7. History of artificial intelligence \n",
            "URL: https://en.wikipedia.org/wiki/History_of_artificial_intelligence\n",
            "8. Artificial Intelligence Act \n",
            "URL: https://en.wikipedia.org/wiki/Artificial_Intelligence_Act\n",
            "9. Artificial intelligence in healthcare \n",
            "URL: https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare\n",
            "10. Artificial intelligence art \n",
            "URL: https://en.wikipedia.org/wiki/Artificial_intelligence_art\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c23a25608b6648b1bda791854b516024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Enter query:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_0d986531f27241e096ff29a639441369",
            "placeholder": "​",
            "style": "IPY_MODEL_d961eadfb879470999ce04bf5de69a38",
            "value": "AI career options"
          }
        },
        "0d986531f27241e096ff29a639441369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d961eadfb879470999ce04bf5de69a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f80f1ed1b9e401fb7867f0ea3d35918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Generate Results",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a02763ef811d4f6bb864d28eae42e26b",
            "style": "IPY_MODEL_a57b5c95054140c6bdec6581a8993809",
            "tooltip": ""
          }
        },
        "a02763ef811d4f6bb864d28eae42e26b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a57b5c95054140c6bdec6581a8993809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}